import streamlit as st
import re
import urllib.parse
from docx import Document
import tempfile
import requests

# Scopus API Key ç®¡ç†
def get_scopus_key():
    try:
        return st.secrets["scopus_api_key"]
    except Exception:
        try:
            with open("scopus_key.txt", "r") as f:
                return f.read().strip()
        except FileNotFoundError:
            st.error("âŒ æ‰¾ä¸åˆ° Scopus API é‡‘é‘°ï¼Œè«‹ç¢ºèªå·²åœ¨ secrets è¨­å®šæˆ–æä¾› scopus_key.txt")
            st.stop()

SCOPUS_API_KEY = get_scopus_key()

# Crossref æŸ¥è©¢
def search_crossref_by_title(title):
    url = "https://api.crossref.org/works"
    params = {
        "query.title": title,
        "rows": 5,
        "mailto": "pauline687@gmail.com"
    }
    response = requests.get(url, params=params)
    if response.status_code == 200:
        items = response.json().get("message", {}).get("items", [])
        for item in items:
            cr_title = item.get("title", [""])[0]
            if title.lower() in cr_title.lower():
                return item.get("URL")
    return None

# Scopus æŸ¥è©¢
def search_scopus_by_title(title):
    base_url = "https://api.elsevier.com/content/search/scopus"
    headers = {
        "Accept": "application/json",
        "X-ELS-APIKey": SCOPUS_API_KEY
    }
    params = {
        "query": f'TITLE("{title}")',
        "count": 5
    }
    response = requests.get(base_url, headers=headers, params=params)
    if response.status_code == 200:
        data = response.json()
        entries = data.get('search-results', {}).get('entry', [])
        for entry in entries:
            doc_title = entry.get('dc:title', '')
            if doc_title.strip().lower() == title.strip().lower():
                return entry.get('prism:url', 'https://www.scopus.com')
    return None

# Word è™•ç†
def extract_paragraphs_from_docx(file):
    doc = Document(file)
    return [para.text.strip() for para in doc.paragraphs if para.text.strip()]

def extract_reference_section(paragraphs, start_keyword):
    start_index = -1
    for i, p in enumerate(paragraphs):
        if start_keyword.lower() in p.lower():
            start_index = i + 1
            break
    return paragraphs[start_index:] if start_index != -1 else []

# æ“·å–æ¨™é¡Œ
def extract_title(ref_text, style):
    if style == "APA":
        match = re.search(r'\(\d{4}\)\.\s(.+?)(\.|\n|$)', ref_text)
        if match:
            return match.group(1).strip()
    elif style == "IEEE":
        match = re.search(r'"(.+?)"', ref_text)
        if match:
            return match.group(1).strip()
    return None

# åˆå§‹åŒ– session state
for key in ["titles", "pending_titles", "scopus_results", "crossref_results"]:
    if key not in st.session_state:
        st.session_state[key] = []

# ä»‹é¢è¨­å®š
st.set_page_config(page_title="Reference Checker", layout="centered")
st.title("ğŸ“š Reference Checker")
st.write("ä¸Šå‚³ Word æª” (.docx)ï¼Œå¾åƒè€ƒæ–‡ç»å€æ“·å–æ¨™é¡Œï¼Œå…ˆæŸ¥ Scopusï¼Œå†æŸ¥ Crossrefï¼ˆé‡å°æŸ¥ä¸åˆ°çš„éƒ¨åˆ†ï¼‰")

# ä¸Šå‚³èˆ‡é¸é …
uploaded_file = st.file_uploader("è«‹ä¸Šå‚³ Word æª”æ¡ˆï¼ˆ.docxï¼‰", type=["docx"])
style = st.selectbox("è«‹é¸æ“‡åƒè€ƒæ–‡ç»æ ¼å¼", ["APA", "IEEE"])
start_keyword = st.selectbox("è«‹é¸æ“‡åƒè€ƒæ–‡ç»èµ·å§‹æ¨™é¡Œ", ["åƒè€ƒæ–‡ç»","References", "Reference"])

# èƒå– Word
if uploaded_file:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    paragraphs = extract_paragraphs_from_docx(tmp_path)
    references = extract_reference_section(paragraphs, start_keyword)

    if not references:
        st.warning("âš ï¸ æ‰¾ä¸åˆ°åƒè€ƒæ–‡ç»æ®µè½ï¼Œè«‹æª¢æŸ¥é—œéµå­—æ˜¯å¦æ­£ç¢ºã€‚")
    else:
        st.session_state.titles = []
        for ref in references:
            title = extract_title(ref, style)
            if title:
                st.session_state.titles.append(title)

# ç¬¬ä¸€æ­¥ï¼šScopus æŸ¥è©¢
if st.session_state.titles and st.button("ğŸ§ª ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ Scopus æŸ¥è©¢"):
    st.subheader("ğŸ” Scopus æŸ¥è©¢çµæœ")
    st.session_state.scopus_results = {}
    st.session_state.pending_titles = []
    for i, title in enumerate(st.session_state.titles):
        url = search_scopus_by_title(title)
        if url:
            st.session_state.scopus_results[title] = url
            st.markdown(f"**{i+1}. {title}**  \nğŸ”— [Scopus æŸ¥è©¢çµæœ]({url})", unsafe_allow_html=True)
        else:
            st.session_state.pending_titles.append(title)
            st.error(f"âš ï¸ ç¬¬ {i+1} ç­†æ‰¾ä¸åˆ° Scopus çµæœï¼š\n> {title}")

# ç¬¬äºŒæ­¥ï¼šCrossref è£œæŸ¥
if st.session_state.pending_titles and st.button("ğŸ” ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ Crossref è£œæŸ¥"):
    st.subheader("ğŸ” Crossref æŸ¥è©¢çµæœï¼ˆé‡å° Scopus æŸ¥ç„¡çµæœï¼‰")
    st.session_state.crossref_results = {}
    for i, title in enumerate(st.session_state.pending_titles):
        url = search_crossref_by_title(title)
        if url:
            st.session_state.crossref_results[title] = url
            st.markdown(f"**{i+1}. {title}**  \nğŸ”— [Crossref æŸ¥è©¢çµæœ]({url})", unsafe_allow_html=True)
        else:
            st.error(f"âŒ Crossref æŸ¥ç„¡çµæœï¼š\n> {title}")

# çµ±æ•´è³‡è¨Š
if st.session_state.titles:
    found = len(st.session_state.scopus_results) + len(st.session_state.crossref_results)
    unresolved = len(st.session_state.titles) - found
    st.markdown("---")
    st.subheader("ğŸ“Š æŸ¥è©¢çµ±è¨ˆçµæœ")
    st.markdown(f"- âœ… æˆåŠŸæŸ¥è©¢çµæœï¼š{found} ç¯‡")
    st.markdown(f"- â“ å°šæœªæŸ¥åˆ°è³‡æ–™ï¼š{unresolved} ç¯‡")

    if unresolved > 0:
        not_found = [t for t in st.session_state.titles if t not in st.session_state.scopus_results and t not in st.session_state.crossref_results]
        with st.expander("â— å¾…æŸ¥æ¨™é¡Œæ¸…å–®"):
            for i, t in enumerate(not_found, 1):
                st.markdown(f"{i}. {t}")
