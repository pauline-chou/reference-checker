import streamlit as st
import re
import urllib.parse
from docx import Document
import requests
from difflib import SequenceMatcher
import pandas as pd
from datetime import datetime
from io import StringIO
from serpapi import GoogleSearch
import fitz 
import re
import unicodedata


# ========== API Key ç®¡ç† ==========
def get_scopus_key():
    try:
        return st.secrets["scopus_api_key"]
    except Exception:
        try:
            with open("scopus_key.txt", "r") as f:
                return f.read().strip()
        except FileNotFoundError:
            st.error("âŒ æ‰¾ä¸åˆ° Scopus API é‡‘é‘°ï¼Œè«‹ç¢ºèªå·²è¨­å®š secrets æˆ–æä¾› scopus_key.txt")
            st.stop()

SCOPUS_API_KEY = get_scopus_key()

def get_serpapi_key():
    try:
        return st.secrets["serpapi_key"]
    except Exception:
        try:
            with open("serpapi_key.txt", "r") as f:
                return f.read().strip()
        except FileNotFoundError:
            st.error("âŒ æ‰¾ä¸åˆ° SerpAPI é‡‘é‘°ï¼Œè«‹ç¢ºèªå·²è¨­å®š secrets æˆ–æä¾› serpapi_key.txt")
            st.stop()

SERPAPI_KEY = get_serpapi_key()

# ========== æ“·å– DOI ==========
def extract_doi(text):
    match = re.search(r'(10\.\d{4,9}/[-._;()/:A-Z0-9]+)', text, re.I)
    if match:
        return match.group(1).rstrip(".")

    doi_match = re.search(r'doi:\s*(https?://doi\.org/)?(10\.\d{4,9}/[-._;()/:A-Z0-9]+)', text, re.I)
    if doi_match:
        return doi_match.group(2).rstrip(".")

    return None

# ========== Crossref DOI æŸ¥è©¢ ==========
def search_crossref_by_doi(doi):
    url = f"https://api.crossref.org/works/{doi}"
    response = requests.get(url)
    if response.status_code == 200:
        item = response.json().get("message", {})
        titles = item.get("title")
        if isinstance(titles, list) and len(titles) > 0:
            return titles[0], item.get("URL")
        else:
            return None, item.get("URL")
    return None, None


# ========================================= æ‰€æœ‰è¦å‰‡å°è£  =========================================
# ========== å¹´ä»½è¦å‰‡ ==========
def is_valid_year(year_str):
    try:
        year = int(year_str)
        return 1000 <= year <= 2050
    except:
        return False
    
# ========== æŠ“é™„éŒ„ ========== 
def is_appendix_heading(text):
    text = text.strip()
    return bool(re.match(
        r'^((\d+|[IVXLCDM]+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åå£¹è²³åƒè‚†ä¼é™¸æŸ’æŒç–æ‹¾]+)[ã€ï¼. ]?\s*)?(é™„éŒ„|APPENDIX)',
        text,
        re.IGNORECASE
    ))

# ========== APAè¦å‰‡ ==========    
def find_apa(ref_text):
    """
    åˆ¤æ–·ä¸€æ®µåƒè€ƒæ–‡ç»æ˜¯å¦ç‚º APA æ ¼å¼ï¼ˆæ¨™æº–æ‹¬è™Ÿå¹´ä»½ or n.d.ï¼‰
    æ¨™æº–æ ¼å¼ï¼šLin, J. (2020). Title.
    æ”¯æ´è®Šé«”ï¼šä¸­è‹±æ–‡æ‹¬è™Ÿã€å¥è™Ÿç¬¦è™Ÿã€n.d. å¹´ä»½
    """
    apa_match = re.search(r'[ï¼ˆ(](\d{4}[a-c]?|n\.d\.)[ï¼‰)]?[ã€‚\.]?', ref_text, re.IGNORECASE)
    if not apa_match:
        return False

    year_str = apa_match.group(1)[:4]
    year_pos = apa_match.start(1)

    # é¿å…åƒ 887(2020) é€™ç¨®å‰æ–¹æ˜¯æ•¸å­—çš„æƒ…æ³
    pre_context = ref_text[max(0, year_pos - 5):year_pos]
    if re.search(r'\d', pre_context):
        return False

    if year_str.isdigit():
        return is_valid_year(year_str)
    return apa_match.group(1).lower() == "n.d."

def match_apa_title_section(ref_text):
    """
    æ“·å– APA çµæ§‹ä¸­çš„æ¨™é¡Œæ®µè½ï¼ˆä½æ–¼å¹´ä»½å¾Œï¼‰
    ç¯„ä¾‹ï¼šLin, J. (2020). Title here.
    - æ”¯æ´æ¨™é»ï¼š.ã€ã€‚ ã€,
    - é¿å…èª¤æŠ“æ•¸å­—ä¸­çš„é€—è™Ÿæˆ–å¥è™Ÿ
    """
    return re.search(
        r'[ï¼ˆ(](\d{4}[a-c]?|n\.d\.)[ï¼‰)]\s*[\.,ï¼Œã€‚]?\s*(.+?)(?:(?<!\d)[,ï¼Œ.ã€‚](?!\d)|$)',
        ref_text,
        re.IGNORECASE
    )

def find_apa_matches(ref_text):
    """
    å›å‚³ç¬¦åˆ APA æ ¼å¼çš„å¹´ä»½ matchï¼ˆå«ä½ç½®ã€åŸæ–‡ç­‰ï¼‰
    """
    APA_PATTERN = r'[ï¼ˆ(](\d{4}[a-c]?|n\.d\.)[ï¼‰)]?[ã€‚\.]?'
    matches = []
    for m in re.finditer(APA_PATTERN, ref_text, re.IGNORECASE):
        year_str = m.group(1)[:4]
        year_pos = m.start(1)
        pre_context = ref_text[max(0, year_pos - 5):year_pos]
        if re.search(r'\d', pre_context):
            continue
        if year_str.isdigit() and is_valid_year(year_str):
            matches.append(m)
        elif m.group(1).lower() == "n.d.":
            matches.append(m)
    return matches


# ========== APA_LIKEè¦å‰‡ ==========
def find_apalike(ref_text):
    valid_years = []

    # é¡å‹ 1ï¼šæ¨™é» + å¹´ä»½ + æ¨™é»ï¼ˆå¸¸è¦‹æ ¼å¼ï¼‰
    for match in re.finditer(r'[,ï¼Œ.ã€‚]\s*(\d{4}[a-c]?)[.ã€‚ï¼Œ]', ref_text):
        year_str = match.group(1)
        year_pos = match.start(1)
        year_core = year_str[:4]
        if not is_valid_year(year_core):
            continue

        # å‰ 5 å­—å…ƒä¸èƒ½æœ‰æ•¸å­—ï¼ˆæ’é™¤ 3.2020. é¡å‹ï¼‰
        pre_context = ref_text[max(0, year_pos - 5):year_pos]
        if re.search(r'\d', pre_context):
            continue

        # è‹¥å¹´ä»½å¾Œ 5 å­—å…ƒæ˜¯ .åŠ æ•¸å­—ï¼Œæˆ–åƒ .v06ã€.abc ç­‰å¸¸è¦‹ DOI çµå°¾ï¼Œå‰‡æ’é™¤
        after_context = ref_text[match.end(1):match.end(1) + 5]
        if re.match(r'\.(\d{1,2}|[a-z0-9]{2,})', after_context, re.IGNORECASE):
            continue

        # æ’é™¤ arXiv å°¾å·´ï¼Œä¾‹å¦‚ arXiv:xxxx.xxxxx, 2023
        arxiv_pattern = re.compile(
            r'arxiv:\d{4}\.\d{5}[^a-zA-Z0-9]{0,3}\s*[,ï¼Œ]?\s*' + re.escape(year_str),
            re.IGNORECASE
        )
        arxiv_match = arxiv_pattern.search(ref_text)
        if arxiv_match and arxiv_match.start() < year_pos:
            continue

        valid_years.append((year_str, year_pos))

    # é¡å‹ 2ï¼šç‰¹æ®Šæ ¼å¼ã€Œï¼Œ2020ï¼Œã€‚ã€ï¼ˆä¸­æ–‡å¸¸è¦‹ï¼‰
    for match in re.finditer(r'ï¼Œ\s*(\d{4}[a-c]?)\s*ï¼Œ\s*ã€‚', ref_text):
        year_str = match.group(1)
        year_pos = match.start(1)
        year_core = year_str[:4]
        if not is_valid_year(year_core):
            continue
        pre_context = ref_text[max(0, year_pos - 5):year_pos]
        if re.search(r'\d', pre_context):
            continue
        valid_years.append((year_str, year_pos))

    return valid_years

def match_apalike_title_section(ref_text):
# é¡å‹ 1ï¼šå¸¸è¦‹æ ¼å¼ï¼ˆ, 2020. Title.ï¼‰
    match = re.search(
        r'[,ï¼Œ.ã€‚]\s*(\d{4}[a-c]?)(?:[.ã€‚ï¼Œ])+\s*(.*?)(?:(?<!\d)[,ï¼Œ.ã€‚](?!\d)|$)',
        ref_text
    )
    if match:
        return match

    # é¡å‹ 2ï¼šç‰¹æ®Šä¸­æ–‡æ ¼å¼ï¼ˆï¼Œ2020ï¼Œã€‚Titleï¼‰
    return re.search(
        r'ï¼Œ\s*(\d{4}[a-c]?)\s*ï¼Œ\s*ã€‚[ \t]*(.+?)(?:[ï¼Œã€‚]|$)',
        ref_text
    )

def find_apalike_matches(ref_text):
    """
    å›å‚³ç¬¦åˆ APA_LIKE æ ¼å¼çš„å¹´ä»½ matchï¼ˆå«ä½ç½®ã€åŸæ–‡ç­‰ï¼‰
    """
    matches = []

    # é¡å‹ 1ï¼šæ¨™é» + å¹´ä»½ + æ¨™é»ï¼ˆå¸¸è¦‹æ ¼å¼ï¼‰
    pattern1 = r'[,ï¼Œ.ã€‚]\s*(\d{4}[a-c]?)[.ã€‚ï¼Œ]'
    for m in re.finditer(pattern1, ref_text):
        year_str = m.group(1)
        year_pos = m.start(1)
        year_core = year_str[:4]
        if not is_valid_year(year_core):
            continue
        pre_context = ref_text[max(0, year_pos - 5):year_pos]
        after_context = ref_text[m.end(1):m.end(1) + 5]
        if re.search(r'\d', pre_context):
            continue
        # æ–°å¢æ¢ä»¶ï¼šå¹´ä»½å¾Œè‹¥æ¥ DOI å‹å¼å‰‡æ’é™¤
        if re.match(r'\.(\d{1,2}|[a-z0-9]{2,})', after_context, re.IGNORECASE):
            continue
        arxiv_pattern = re.compile(
            r'arxiv:\d{4}\.\d{5}[^a-zA-Z0-9]{0,3}\s*[,ï¼Œ]?\s*' + re.escape(year_str),
            re.IGNORECASE
        )
        if arxiv_pattern.search(ref_text) and arxiv_pattern.search(ref_text).start() < year_pos:
            continue
        matches.append(m)

    # é¡å‹ 2ï¼šç‰¹æ®Šä¸­æ–‡æ ¼å¼ã€Œï¼Œ2020ï¼Œã€‚ã€
    pattern2 = r'ï¼Œ\s*(\d{4}[a-c]?)\s*ï¼Œ\s*ã€‚'
    for m in re.finditer(pattern2, ref_text):
        year_str = m.group(1)
        year_pos = m.start(1)
        year_core = year_str[:4]  # âœ… è£œä¸Š year_core
        pre_context = ref_text[max(0, year_pos - 5):year_pos]
        if re.search(r'\d', pre_context):
            continue
        if is_valid_year(year_core):
            matches.append(m)

    return matches


# ================================================================================================


# ========== æ¸…æ´—æ¨™é¡Œ ==========
def clean_title(text):
    # ç§»é™¤ dash é¡ç¬¦è™Ÿ
    dash_variants = ["-", "â€“", "â€”", "âˆ’", "â€‘", "â€"]
    for d in dash_variants:
        text = text.replace(d, "")

    # æ¨™æº–åŒ–å­—ç¬¦ï¼ˆä¾‹å¦‚å…¨å½¢è½‰åŠå½¢ï¼‰
    text = unicodedata.normalize('NFKC', text)

    # éæ¿¾æ‰æ¨™é»ç¬¦è™Ÿã€ç¬¦è™Ÿé¡åˆ¥ï¼ˆä¸åˆªæ–‡å­—ï¼ï¼‰
    cleaned = []
    for ch in text:
        if unicodedata.category(ch)[0] in ("L", "N", "Z"):  # L=Letter, N=Number, Z=Space
            cleaned.append(ch.lower())
        # else: è·³éæ¨™é»èˆ‡ç¬¦è™Ÿ

    # çµ±ä¸€ç©ºç™½
    return re.sub(r'\s+', ' ', ''.join(cleaned)).strip()

# å°ˆé–€çµ¦è£œæ•‘å‘½ä¸­çš„æ¸…æ´—
def clean_title_for_remedial(text):
    """çµ¦è£œæ•‘æŸ¥è©¢ç”¨çš„æ¸…æ´—ï¼šå»æ‰å–®ç¨æ•¸å­—ã€æ¨™é»ã€å…¨å½¢è½‰åŠå½¢ç­‰"""
    # æ¨™æº–åŒ–å­—å…ƒï¼ˆå…¨å½¢è½‰åŠå½¢ï¼‰
    text = unicodedata.normalize('NFKC', text)

    # ç§»é™¤ dash é¡ç¬¦è™Ÿ
    dash_variants = ["-", "â€“", "â€”", "âˆ’", "â€‘", "â€"]
    for d in dash_variants:
        text = text.replace(d, "")

    # ç§»é™¤å–®ç¨çš„æ•¸å­—è©ï¼ˆå¦‚é ç¢¼ã€å·è™Ÿï¼‰
    text = re.sub(r'\b\d+\b', '', text)

    # ä¿ç•™å­—æ¯ã€æ•¸å­—ã€ç©ºç™½
    cleaned = []
    for ch in text:
        if unicodedata.category(ch)[0] in ("L", "N", "Z"):  # L=Letter, N=Number, Z=Space
            cleaned.append(ch.lower())

    return re.sub(r'\s+', ' ', ''.join(cleaned)).strip()

# ========== Scopus æŸ¥è©¢ ==========
def search_scopus_by_title(title):
    base_url = "https://api.elsevier.com/content/search/scopus"
    headers = {
        "Accept": "application/json",
        "X-ELS-APIKey": SCOPUS_API_KEY
    }
    params = {
        "query": f'TITLE("{title}")',
        "count": 3
    }
    response = requests.get(base_url, headers=headers, params=params)
    if response.status_code == 200:
        data = response.json()
        entries = data.get('search-results', {}).get('entry', [])
        for entry in entries:
            doc_title = entry.get('dc:title', '')
            if doc_title.strip().lower() == title.strip().lower():
                return entry.get('prism:url', 'https://www.scopus.com')
    return None

# ========== Serpapi æŸ¥è©¢ ==========
def search_scholar_by_title(title, api_key, threshold=0.90):
    search_url = f"https://scholar.google.com/scholar?q={urllib.parse.quote(title)}"
    params = {
        "engine": "google_scholar",
        "q": title,
        "api_key": api_key,
        "num": 3
    }

    try:
        results = GoogleSearch(params).get_dict()

        if "error" in results:
            error_msg = results["error"]
            st.session_state["serpapi_error"] = error_msg
            return search_url, "error"

        organic = results.get("organic_results", [])
        if not organic:
            return search_url, "no_result"

        cleaned_query = clean_title(title)
        for result in organic:
            result_title = result.get("title", "")
            cleaned_result = clean_title(result_title)

            if not cleaned_query or not cleaned_result:
                continue

            if cleaned_query == cleaned_result:
                return search_url, "match"
            if SequenceMatcher(None, cleaned_query, cleaned_result).ratio() >= threshold:
                return search_url, "similar"

        return search_url, "no_result"

    except Exception as e:
        st.session_state["serpapi_error"] = f"API æŸ¥è©¢éŒ¯èª¤ï¼š{e}"
        return search_url, "error"


#è£œæ•‘æœå°‹
def search_scholar_by_ref_text(ref_text, api_key):
    search_url = f"https://scholar.google.com/scholar?q={urllib.parse.quote(ref_text)}"
    params = {
        "engine": "google_scholar",
        "q": ref_text,
        "api_key": api_key,
        "num": 1
    }

    try:
        results = GoogleSearch(params).get_dict()
        organic = results.get("organic_results", [])
        if not organic:
            return search_url, "no_result"

        first_title = organic[0].get("title", "")

        # ä½¿ç”¨ä¹¾æ·¨ç‰ˆæ¸…æ´—ï¼ˆä¸å½±éŸ¿ä¸»æµç¨‹ï¼‰
        cleaned_ref = clean_title_for_remedial(ref_text)
        cleaned_first = clean_title_for_remedial(first_title)

        if cleaned_first in cleaned_ref or cleaned_ref in cleaned_first:
            return search_url, "remedial"

        return search_url, "no_result"

    except Exception as e:
        return search_url, "no_result"

    
# ========== Word è™•ç† ==========
def extract_paragraphs_from_docx(file):
    # ä½¿ç”¨ BytesIO è™•ç† UploadedFile
    doc = Document(file)
    return [para.text.strip() for para in doc.paragraphs if para.text.strip()]

# ========== PDF è™•ç† ==========
def extract_paragraphs_from_pdf(file):
    text = ""
    with fitz.open(stream=file.read(), filetype="pdf") as doc:
        for page in doc:
            page_text = page.get_text("text")
            text += page_text + "\n"
    paragraphs = [p.strip() for p in text.split("\n") if p.strip()]
    return paragraphs

# ========== èƒå–åƒè€ƒæ–‡ç» ==========
def extract_reference_section_from_bottom(paragraphs, start_keywords=None):
    """
    å¾åº•éƒ¨å¾€ä¸Šæ‰¾å‡ºåƒè€ƒæ–‡ç»å€æ®µèµ·é»ï¼Œä¸¦å‘ä¸‹æ“·å–è‡³é‡åˆ°åœæ­¢æ¨™é¡Œï¼ˆå¦‚é™„éŒ„ï¼‰ç‚ºæ­¢
    å›å‚³æ ¼å¼ï¼šmatched_section, matched_keyword
    """
    if start_keywords is None:
        start_keywords = [
            "åƒè€ƒæ–‡ç»", "åƒè€ƒè³‡æ–™", "references", "reference",
            "bibliography", "works cited", "literature cited",
            "references and citations"
        ]

    for i in reversed(range(len(paragraphs))):
        para = paragraphs[i].strip()

        # è·³éå¤ªé•·æˆ–åŒ…å«æ¨™é»çš„æ®µè½ï¼ˆå¯èƒ½æ˜¯æ­£æ–‡ï¼‰
        if len(para) > 30 or re.search(r'[.,;:]', para):
            continue

        normalized = para.lower()
        if normalized in start_keywords:
            # å¾ i+1 é–‹å§‹æ“·å–ï¼Œç›´åˆ°é‡åˆ°é™„éŒ„ç‚ºæ­¢
            result = []
            for p in paragraphs[i + 1:]:
                if is_appendix_heading(p):
                    break
                result.append(p)
            return result, para

    return [], None



# ========== èƒå–åƒè€ƒæ–‡ç» (åŠ å¼·ç‰ˆ) ==========
#ä¹Ÿæ˜¯éœ€è¦æŠŠé™„éŒ„æˆªæ‰
def clip_until_stop(paragraphs_after):
    result = []
    for para in paragraphs_after:
        if is_appendix_heading(para):
            break
        result.append(para)
    return result

def extract_reference_section_improved(paragraphs):
    """
    æ”¹é€²çš„åƒè€ƒæ–‡ç»å€æ®µè­˜åˆ¥ï¼Œå¾åº•éƒ¨å¾€ä¸Šæƒæï¼Œä½¿ç”¨å¤šé‡ç­–ç•¥å’Œå®¹éŒ¯æ©Ÿåˆ¶
    è¿”å›ï¼š(åƒè€ƒæ–‡ç»æ®µè½åˆ—è¡¨, è­˜åˆ¥åˆ°çš„æ¨™é¡Œ, è­˜åˆ¥æ–¹æ³•)
    """

    def is_reference_format(text):
        text = text.strip()
        if len(text) < 10:
            return False
        if re.search(r'\(\d{4}[a-c]?\)', text):  # APA å¹´ä»½æ ¼å¼
            return True
        if re.match(r'^\[\d+\]', text):         # IEEE ç·¨è™Ÿæ ¼å¼
            return True
        if re.search(r'[A-Z][a-z]+,\s*[A-Z]\.', text):  # ä½œè€…åæ¨£å¼
            return True
        return False

    reference_keywords = [
        "åƒè€ƒæ–‡ç»", "references", "reference",
        "bibliography", "works cited", "literature cited",
        "references and citations", "åƒè€ƒæ–‡ç»æ ¼å¼"
    ]

    # âœ… å¾åº•éƒ¨å¾€ä¸Šæƒæ
    for i in reversed(range(len(paragraphs))):
        para = paragraphs[i].strip()
        para_lower = para.lower()
        para_nospace = re.sub(r'\s+', '', para_lower)

        # âœ… ç´”æ¨™é¡Œç›¸ç¬¦ï¼ˆe.g. "References"ï¼‰
        if para_lower in reference_keywords:
            return clip_until_stop(paragraphs[i + 1:]), para, "ç´”æ¨™é¡Œè­˜åˆ¥ï¼ˆåº•éƒ¨ï¼‰"

        # âœ… å®¹éŒ¯æ¨™é¡Œï¼ˆå«ç« ç¯€ç·¨è™Ÿï¼‰
        if re.match(
            r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬å£¹è²³åƒè‚†ä¼é™¸æŸ’æŒç–æ‹¾ç™¾åƒè¬]+ç« [ã€.ï¼]?\s*)?(åƒè€ƒæ–‡ç»|references?|bibliography|works cited|literature cited|references and citations)\s*$',
            para_lower
        ):


            return clip_until_stop(paragraphs[i + 1:]), para, "ç« ç¯€æ¨™é¡Œè­˜åˆ¥ï¼ˆåº•éƒ¨ï¼‰"

        # âœ… æ¨¡ç³Šé—œéµå­— + å¾Œé¢æ®µè½åƒ APA æ ¼å¼
        fuzzy_keywords = ["reference", "åƒè€ƒ", "bibliography", "æ–‡ç»", " REFERENCES AND CITATIONS"]
        if any(para_lower.strip() == k for k in fuzzy_keywords):  # â— åªæ¥å—æ•´è¡Œå‰›å¥½ç­‰æ–¼é—œéµå­—
            if i + 1 < len(paragraphs):
                next_paras = paragraphs[i+1:i+6]
                if sum(1 for p in next_paras if is_reference_format(p)) >= 1:
                    return clip_until_stop(paragraphs[i + 1:]), para.strip(), "æ¨¡ç³Šæ¨™é¡Œ+å…§å®¹è­˜åˆ¥"



    return [], None, "æœªæ‰¾åˆ°åƒè€ƒæ–‡ç»å€æ®µ"







# ========== åµæ¸¬æ ¼å¼ ==========
def detect_reference_style(ref_text):
    # IEEE é€šå¸¸é–‹é ­æ˜¯ [1]ï¼Œæˆ–å«æœ‰è‹±æ–‡å¼•è™Ÿ "æ¨™é¡Œ"
    if re.match(r'^\[\d+\]', ref_text) or '"' in ref_text:
        return "IEEE"

    # APAï¼šä½¿ç”¨å°è£å¾Œçš„ find_apa()
    if find_apa(ref_text):
        return "APA"

    # APA_LIKEï¼šä½¿ç”¨å°è£å¾Œçš„ find_apalike()
    if find_apalike(ref_text):
        return "APA_LIKE"

    return "Unknown"

# ========== æ®µè½åˆä½µå™¨ï¼ˆPDF å°ˆç”¨ï¼Œæ ¹æ“šåƒè€ƒæ–‡ç»é–‹é ­åˆ‡åˆ†ï¼‰ ==========
def is_reference_head(para):
    """
    åˆ¤æ–·æ®µè½æ˜¯å¦ç‚ºåƒè€ƒæ–‡ç»é–‹é ­ï¼ˆAPAã€APA_LIKE æˆ– IEEEï¼‰
    """
    # APAï¼šä½¿ç”¨å°è£å¥½çš„åˆ¤æ–·
    if find_apa(para):
        return True

    # IEEEï¼šé–‹é ­ç‚º [æ•¸å­—]
    if re.match(r"^\[\d+\]", para):
        return True

    # APA_LIKEï¼šä½¿ç”¨å°è£å¥½çš„åˆ¤æ–·
    if find_apalike(para):
        return True

    return False

def detect_and_split_ieee(paragraphs):
    """
    è‹¥ç¬¬ä¸€æ®µç‚º IEEE æ ¼å¼ [1] é–‹é ­ï¼Œå‰‡å°‡æ•´æ®µåˆä½µä¸¦ä¾æ“š [æ•¸å­—] åˆ‡å‰²
    """
    if not paragraphs:
        return None

    first_line = paragraphs[0].strip()
    if not re.match(r'^\[\d+\]', first_line):
        return None

    full_text = ' '.join(paragraphs)  # å°‡æ›è¡Œè¦–ç‚ºç©ºæ ¼
    refs = re.split(r'(?=\[\d+\])', full_text)  # ç”¨ lookahead ä¿ç•™åˆ‡å‰²é»
    return [r.strip() for r in refs if r.strip()]

def merge_references_by_heads(paragraphs):
    merged = []

    for para in paragraphs:
        # ä½¿ç”¨å°è£å¥½çš„ APA åˆ¤æ–·
        apa_count = 1 if find_apa(para) else 0

        # ä½¿ç”¨å°è£å¥½çš„ APA_LIKE åˆ¤æ–·ï¼ˆå›å‚³å¤šå€‹å¹´ä»½ä½ç½®ï¼‰
        apalike_count = len(find_apalike(para))

        if apa_count >= 2 or apalike_count >= 2:
            sub_refs = split_multiple_apa_in_paragraph(para)
            merged.extend([s.strip() for s in sub_refs if s.strip()])
        else:
            if is_reference_head(para):
                merged.append(para.strip())
            else:
                if merged:
                    merged[-1] += " " + para.strip()
                else:
                    merged.append(para.strip())

    return merged


def split_multiple_apa_in_paragraph(paragraph):
    """
    æ”¹è‰¯ç‰ˆï¼šå¾å‡ºç¾ç¬¬ 2 ç­† APA æˆ– APA_LIKE å¹´ä»½èµ·ï¼Œæ¯ç­†å¾€å‰å›ºå®š 5 å­—å…ƒåˆ‡æ®µã€‚
    - APAï¼š (2020)ã€(2020a)ã€(n.d.)
    - APA_LIKEï¼š , 2020. æˆ– .2020. ç­‰ï¼Œä¸”å‰ 5 å­—å…ƒä¸èƒ½å«æ•¸å­—
    """

    # ä½¿ç”¨çµ±ä¸€å°è£å‡½æ•¸æ‰¾å‡ºæ‰€æœ‰ APA èˆ‡ APA_LIKE çš„ matches
    apa_matches = find_apa_matches(paragraph)
    apalike_matches = find_apalike_matches(paragraph)

    all_matches = apa_matches + apalike_matches
    all_matches.sort(key=lambda m: m.start())

    # è‹¥ä¸åˆ° 2 ç­†å‰‡ä¸åˆ‡
    if len(all_matches) < 2:
        return [paragraph]

    # æ¯ç­†å¾å‰é¢å›ºå®šå›æ¨ 5 å­—å…ƒåˆ‡å‰²
    split_indices = []
    for match in all_matches[1:]:  # å¾ç¬¬ 2 ç­†é–‹å§‹åˆ‡
        cut_index = max(0, match.start() - 5)
        split_indices.append(cut_index)

    segments = []
    start = 0
    for idx in split_indices:
        segments.append(paragraph[start:idx].strip())
        start = idx
    segments.append(paragraph[start:].strip())

    return [s for s in segments if s]



# ========== æ“·å–æ¨™é¡Œ ==========
def extract_title(ref_text, style):
    if style == "APA":
        match = match_apa_title_section(ref_text)
        if match:
            year_str = match.group(1)[:4]
            if year_str.isdigit() and not is_valid_year(year_str):
                return None
            return match.group(2).strip(" ,ã€‚")

    elif style == "IEEE":
        matches = re.findall(r'"([^"]+)"', ref_text)
        if matches:
            return max(matches, key=len).strip().rstrip(",.")
        fallback = re.search(r'(?<!et al)([A-Z][^,.]+[a-zA-Z])[,\.]', ref_text)
        if fallback:
            return fallback.group(1).strip(" ,.")

    elif style == "APA_LIKE":
        match = match_apalike_title_section(ref_text)
        if match:
            year_str = match.group(1)
            after_fragment = ref_text[match.end(1):match.end(1)+5]
            if is_valid_year(year_str) and not re.match(r'\.\d', after_fragment):
                return match.group(2).strip(" ,ã€‚")

    return None



# ========== åˆ†æå–®ç­†åƒè€ƒæ–‡ç»ç”¨ï¼ˆå« APA_LIKE å¹´ä»½çµ±è¨ˆï¼‰ ==========
def analyze_single_reference(ref_text, ref_index):
    style = detect_reference_style(ref_text)
    title = extract_title(ref_text, style)
    doi = extract_doi(ref_text)

    # APA èˆ‡ APA_LIKE å¹´ä»½æ¨™è¨»ï¼ˆé«˜äº®ï¼‰
    highlights = ref_text
    # æ‰€æœ‰ match çµ±ä¸€åŠ å…¥ï¼Œä¸¦æ ¹æ“šä½ç½®å¾å¾Œå¾€å‰é«˜äº®ï¼Œé¿å…é‡ç–Š offset éŒ¯äº‚
    all_year_matches = find_apa_matches(ref_text) + find_apalike_matches(ref_text)
    all_year_matches.sort(key=lambda m: m.start(), reverse=True)
    for match in all_year_matches:
        start, end = match.span()
        highlights = highlights[:start] + "**" + highlights[start:end] + "**" + highlights[end:]

    # === å¹´ä»½çµ±è¨ˆ ===
    apa_year_count = len(find_apa_matches(ref_text))
    apalike_year_count = len(find_apalike_matches(ref_text))
    year_count = apa_year_count + apalike_year_count

    # === è¼¸å‡ºåˆ° UI ===
    st.markdown(f"**{ref_index}.**")
    st.write(highlights)
    st.markdown(f"""
    â€¢ ğŸ“° **æ“·å–æ¨™é¡Œ**ï¼š{title if title else "âŒ ç„¡æ³•æ“·å–"}  
    â€¢ ğŸ” **æ“·å– DOI**ï¼š{doi if doi else "âŒ ç„¡ DOI"}  
    â€¢ ğŸ·ï¸ **åµæ¸¬é¢¨æ ¼**ï¼š{style}  
    â€¢ ğŸ“… **å¹´ä»½å‡ºç¾æ¬¡æ•¸**ï¼š{year_count}  
    """)

    return (ref_text, title) if title else None



# ========== Streamlit UI ==========
st.set_page_config(page_title="Reference Checker", layout="centered")
if "start_query" not in st.session_state:
    st.session_state.start_query = False
if "query_results" not in st.session_state:
    st.session_state.query_results = None
st.title("ğŸ“š Reference Checker")

st.markdown("""
<div style="background-color: #fff9db; padding: 15px; border-left: 6px solid #f1c40f; border-radius: 6px;">
    <span style="font-size: 16px; font-weight: bold;">æ³¨æ„äº‹é …</span><br>
    <span style="font-size: 15px; color: #444;">
    ç‚ºç¯€çœæ ¸å°æ™‚é–“ï¼Œæœ¬ç³»çµ±åªæŸ¥å°æœ‰ DOI ç¢¼çš„æœŸåˆŠè«–æ–‡ã€‚ä¸¦æœªæª¢æŸ¥æœŸåˆŠåç¨±ã€ä½œè€…ã€å·æœŸã€é ç¢¼ï¼Œåƒ…é‡å°ç¯‡åé€²è¡Œæ ¸å°ã€‚æœ¬ç³»çµ±åƒ…æä¾›åˆæ­¥ç¯©é¸åƒè€ƒï¼Œæ¯”å°å¾Œæ‡‰é€²è¡Œäººå·¥æ ¸å°ï¼Œä¸å¾—ç›´æ¥ä»¥æœ¬ç³»çµ±æ ¸å°çµæœä½œç‚ºå­¸è¡“å€«ç†åˆ¤æ–·çš„ä¾æ“šã€‚
    </span>
</div>
""", unsafe_allow_html=True)
st.markdown(" ")

uploaded_files = st.file_uploader("è«‹ä¸Šå‚³æœ€å¤š 10 å€‹ Word æˆ– PDF æª”æ¡ˆ", type=["docx", "pdf"], accept_multiple_files=True)
# æ””æˆªè¶…é 10 æª”æ¡ˆçš„æƒ…æ³
if uploaded_files and len(uploaded_files) > 10:
    st.error("âŒ ä¸Šå‚³æª”æ¡ˆè¶…é 10 å€‹ï¼Œè«‹åˆªé™¤éƒ¨åˆ†æª”æ¡ˆå¾Œå†è©¦ä¸€æ¬¡ã€‚")
    st.stop()

start_button = st.button("ğŸš€ é–‹å§‹æŸ¥è©¢")

if uploaded_files and start_button:
    st.subheader("ğŸ“Š æ­£åœ¨æŸ¥è©¢ä¸­ï¼Œè«‹ç¨å€™...")

    all_results = []

    for uploaded_file in uploaded_files:
        file_ext = uploaded_file.name.split(".")[-1].lower()
        st.markdown(f"ğŸ“„ è™•ç†æª”æ¡ˆï¼š {uploaded_file.name}")

        file_progress = st.progress(0.0)
        scholar_logs = []

        # æª”æ¡ˆè§£æ
        if file_ext == "docx":
            paragraphs = extract_paragraphs_from_docx(uploaded_file)
        elif file_ext == "pdf":
            paragraphs = extract_paragraphs_from_pdf(uploaded_file)
        else:
            st.warning(f"âš ï¸ æª”æ¡ˆ {uploaded_file.name} æ ¼å¼ä¸æ”¯æ´ï¼Œå°‡ç•¥éã€‚")
            continue

        # ========== æ“·å–åƒè€ƒæ–‡ç»å€æ®µï¼šå…ˆè·‘åŠ å¼·ç‰ˆï¼Œæ‰¾ä¸åˆ°å† fallback ==========
        matched_section, matched_keyword, matched_method = extract_reference_section_improved(paragraphs)

        if not matched_section:
            matched_section, matched_keyword = extract_reference_section_from_bottom(paragraphs)
            matched_method = "æ¨™æº–æ¨™é¡Œè­˜åˆ¥ï¼ˆåº•éƒ¨ï¼‰"

        if not matched_section:
            st.error(f"âŒ ç„¡æ³•è­˜åˆ¥æª”æ¡ˆ {uploaded_file.name} çš„åƒè€ƒæ–‡ç»å€æ®µï¼Œå·²è·³éè©²æª”æ¡ˆã€‚")
            continue




        with st.expander("æ“·å–åˆ°çš„åƒè€ƒæ–‡ç»æ®µè½ï¼ˆä¾›äººå·¥æª¢æŸ¥ï¼‰"):
            st.markdown(f"åƒè€ƒæ–‡ç»æ®µè½åµæ¸¬æ–¹å¼ï¼š**{matched_method}**")
            st.markdown(f"èµ·å§‹é—œéµæ®µè½ï¼š**{matched_keyword}**")
            for i, para in enumerate(matched_section, 1):
                st.markdown(f"**{i}.** {para}")

        # åˆä½µ
        if file_ext == "pdf":
            ieee_refs = detect_and_split_ieee(matched_section)
            merged_references = ieee_refs if ieee_refs else merge_references_by_heads(matched_section)
        else:
            merged_references = matched_section

        # è£œä¸ï¼šè‹¥ç¬¬ä¸€ç­†ç‚º Unknown æ ¼å¼ï¼Œåˆä½µç¬¬ä¸€ã€äºŒç­†æ®µè½
        if len(merged_references) >= 2:
            first_style = detect_reference_style(merged_references[0])
            if first_style == "Unknown":
                merged_references[0] = merged_references[0].strip() + " " + merged_references[1].strip()
                del merged_references[1]  # åˆªé™¤åŸç¬¬äºŒç­†


        title_pairs = []
        with st.expander("é€ç­†åƒè€ƒæ–‡ç»è§£æçµæœï¼ˆåˆä½µå¾Œæ®µè½ + æ¨™é¡Œ + DOI + æ ¼å¼ï¼‰"):
            ref_index = 1
            for para in merged_references:
                # çµ±ä¸€å–å¾— APA å’Œ APA_LIKE æ‰€æœ‰å¹´ä»½ match
                apa_matches = find_apa_matches(para)
                apalike_matches = find_apalike_matches(para)
                total_valid_years = len(apa_matches) + len(apalike_matches)

                if total_valid_years >= 2:
                    sub_refs = split_multiple_apa_in_paragraph(para)
                    st.markdown(f"ğŸ” å¼·åˆ¶åˆ‡åˆ†æ®µè½ï¼ˆåŸå§‹æ®µè½å« {total_valid_years} å€‹å¹´ä»½ï¼‰ï¼š")
                    for sub_ref in sub_refs:
                        result = analyze_single_reference(sub_ref, ref_index)
                        if result:
                            title_pairs.append(result)
                        ref_index += 1
                else:
                    result = analyze_single_reference(para, ref_index)
                    if result:
                        title_pairs.append(result)
                    ref_index += 1


        # æŸ¥è©¢
        crossref_doi_hits = {}
        scopus_hits = {}
        scholar_hits = {}
        scholar_similar = {}
        scholar_remedial = {}
        not_found = []

        for i, (ref, title) in enumerate(title_pairs, 1):
            doi = extract_doi(ref)
            if doi:
                title_from_doi, url = search_crossref_by_doi(doi)
                if title_from_doi:
                    crossref_doi_hits[ref] = url
                    file_progress.progress(i / len(title_pairs))
                    continue

            url = search_scopus_by_title(title)
            if url:
                scopus_hits[ref] = url
            else:
                gs_url, gs_type = search_scholar_by_title(title, SERPAPI_KEY)
                scholar_logs.append(f"Google Scholar å›å‚³é¡å‹ï¼š{gs_type} / æ¨™é¡Œï¼š{title}")
                if gs_type == "match":
                    scholar_hits[ref] = gs_url
                elif gs_type == "similar":
                    scholar_similar[ref] = gs_url
                elif gs_type == "error":
                    not_found.append(ref)
                else:
                    remedial_url, remedial_type = search_scholar_by_ref_text(ref, SERPAPI_KEY)
                    scholar_logs.append(f"Google Scholar å›å‚³é¡å‹ï¼šremedial_{remedial_type} / æ¨™é¡Œï¼š{title}")
                    if remedial_type == "remedial":
                        scholar_remedial[ref] = remedial_url
                    else:
                        not_found.append(ref)

            file_progress.progress(i / len(title_pairs))

        if scholar_logs:
            with st.expander("Google Scholar æŸ¥è©¢éç¨‹ç´€éŒ„"):
                for line in scholar_logs:
                    st.text(line)

        # æ¯å€‹æª”æ¡ˆéƒ½è¨˜éŒ„çµæœ
        file_results = {
            "filename": uploaded_file.name,
            "title_pairs": title_pairs,
            "crossref_doi_hits": crossref_doi_hits,
            "scopus_hits": scopus_hits,
            "scholar_hits": scholar_hits,
            "scholar_similar": scholar_similar,
            "scholar_remedial": scholar_remedial,
            "not_found": not_found,
            "report_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        all_results.append(file_results)

    # æª”æ¡ˆè™•ç†å®Œç•¢ï¼Œå„²å­˜è‡³ session
    st.session_state.query_results = all_results

# å¦‚æœ SerpAPI ç”¨é‡å·²è¶…éï¼Œé¡¯ç¤ºä¸€æ¬¡æ€§æç¤º
if st.session_state.get("serpapi_exceeded"):
    st.warning("âš ï¸ SerpAPI æŸ¥è©¢é¡åº¦å·²ç”¨å®Œï¼Œå› æ­¤éƒ¨åˆ†çµæœå¯èƒ½ç„¡æ³•å¾ Google Scholar æŸ¥å¾—ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–ç¢ºèª API ä½¿ç”¨ç‹€æ³ã€‚")
elif st.session_state.get("serpapi_error"):
    st.warning(f"âš ï¸ Google Scholar æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{st.session_state['serpapi_error']}")




# ========== ä¸Šå‚³ä¸¦è™•ç† ==========


if st.session_state.query_results:
        st.markdown("---")
        st.subheader("ğŸ“Š æŸ¥è©¢çµæœåˆ†é¡")
        for result in st.session_state.query_results:
            not_found = result.get("not_found", [])
            title_pairs = result.get("title_pairs", [])
            crossref_doi_hits = result.get("crossref_doi_hits", {})
            scholar_similar = result.get("scholar_similar", {})
            scholar_remedial = result.get("scholar_remedial", {})
            uploaded_filename = result.get("filename", "æœªçŸ¥æª”æ¡ˆ")
            report_time = result.get("report_time", "æœªè¨˜éŒ„")
            scopus_hits = result.get("scopus_hits", {})
            scholar_hits = result.get("scholar_hits", {})
            

            st.markdown(f"ğŸ“„ æª”æ¡ˆåç¨±ï¼š {uploaded_filename}")
            matched_count = len(crossref_doi_hits) + len(scopus_hits) + len(scholar_hits) + len(scholar_remedial)
            hit_tab, similar_tab, miss_tab = st.tabs([
                f"ğŸŸ¢ å‘½ä¸­çµæœï¼ˆ{matched_count}ï¼‰",
                f"ğŸŸ¡ Google Scholar é¡ä¼¼æ¨™é¡Œï¼ˆ{len(scholar_similar)}ï¼‰",
                f"ğŸ”´ å‡æŸ¥ç„¡çµæœï¼ˆ{len(not_found)}ï¼‰"
            ])

            with hit_tab:
                if crossref_doi_hits:
                    with st.expander(f"\U0001F7E2 Crossref DOI å‘½ä¸­ï¼ˆ{len(crossref_doi_hits)}ï¼‰"):
                        for i, (title, url) in enumerate(crossref_doi_hits.items(), 1):
                            st.markdown(f"{i}. {title}  \nğŸ”— [DOI é€£çµ]({url})", unsafe_allow_html=True)

                if scopus_hits:
                    with st.expander(f"\U0001F7E2 Scopus æ¨™é¡Œå‘½ä¸­ï¼ˆ{len(scopus_hits)}ï¼‰"):
                        for i, (title, url) in enumerate(scopus_hits.items(), 1):
                            st.markdown(f"{i}. {title}  \nğŸ”— [Scopus é€£çµ]({url})", unsafe_allow_html=True)

                if scholar_hits:
                    with st.expander(f"\U0001F7E2 Google Scholar æ¨™é¡Œå‘½ä¸­ï¼ˆ{len(scholar_hits)}ï¼‰"):
                        for i, (title, url) in enumerate(scholar_hits.items(), 1):
                            st.markdown(f"{i}. {title}  \nğŸ”— [Scholar é€£çµ]({url})", unsafe_allow_html=True)
                if scholar_remedial:
                    with st.expander(f"\U0001F7E2 Google Scholar è£œæ•‘å‘½ä¸­ï¼ˆ{len(scholar_remedial)}ï¼‰"):
                        for i, (title, url) in enumerate(scholar_remedial.items(), 1):
                            st.markdown(f"{i}. {title}  \nğŸ”— [Scholar é€£çµ]({url})", unsafe_allow_html=True)
                
                if not (crossref_doi_hits or scopus_hits or scholar_hits):
                    st.info("æ²’æœ‰å‘½ä¸­ä»»ä½•åƒè€ƒæ–‡ç»ã€‚")

            with similar_tab:
                if scholar_similar:
                    for i, (title, url) in enumerate(scholar_similar.items(), 1):
                        with st.expander(f"{i}. {title}"):
                            st.markdown(f"ğŸ”— [Google Scholar çµæœé€£çµ]({url})", unsafe_allow_html=True)
                            st.warning("âš ï¸ æ­¤ç‚ºç›¸ä¼¼æ¨™é¡Œï¼Œè«‹äººå·¥ç¢ºèªæ˜¯å¦ç‚ºæ­£ç¢ºæ–‡ç»ã€‚")
                else:
                    st.info("ç„¡æ¨™é¡Œç›¸ä¼¼ä½†ä¸ä¸€è‡´çš„çµæœã€‚")

            with miss_tab:
                if not_found:
                    for i, title in enumerate(not_found, 1):
                        scholar_url = f"https://scholar.google.com/scholar?q={urllib.parse.quote(title)}"
                        st.markdown(f"{i}. {title}  \nğŸ”— [Google Scholar æœå°‹]({scholar_url})", unsafe_allow_html=True)
                    st.markdown("ğŸ‘‰ è«‹è€ƒæ…®æ‰‹å‹•æœå°‹ Google Scholarã€‚")
                else:
                    st.success("æ‰€æœ‰æ¨™é¡Œçš†æˆåŠŸæŸ¥è©¢ï¼")

        # ä¸‹è¼‰çµæœ
        st.markdown("---")

        export_data = []
        for result in st.session_state.query_results:
            filename = result["filename"]
            for ref, title in result["title_pairs"]:
                if ref in result["crossref_doi_hits"]:
                    export_data.append([filename, ref, "Crossref æœ‰ DOI è³‡è¨Š", result["crossref_doi_hits"][ref]])
                elif ref in result["scopus_hits"]:
                    export_data.append([filename, ref, "æ¨™é¡Œå‘½ä¸­ï¼ˆScopusï¼‰", result["scopus_hits"][ref]])
                elif ref in result["scholar_hits"]:
                    export_data.append([filename, ref, "æ¨™é¡Œå‘½ä¸­ï¼ˆGoogle Scholarï¼‰", result["scholar_hits"][ref]])
                elif ref in result["scholar_similar"]:
                    export_data.append([filename, ref, "Google Scholar é¡ä¼¼æ¨™é¡Œ", result["scholar_similar"][ref]])
                elif ref in result.get("scholar_remedial", {}):
                    export_data.append([filename, ref, "Google Scholar è£œæ•‘å‘½ä¸­", result["scholar_remedial"][ref]])
                elif ref in result["not_found"]:
                    scholar_url = f"https://scholar.google.com/scholar?q={urllib.parse.quote(ref)}"
                    export_data.append([filename, ref, "æŸ¥ç„¡çµæœ", scholar_url])
        total_refs = sum(len(r["title_pairs"]) for r in st.session_state.query_results)
        matched_exact = sum(len(r["crossref_doi_hits"]) + len(r["scopus_hits"]) + len(r["scholar_hits"]) for r in st.session_state.query_results)
        matched_similar = sum(len(r["scholar_similar"]) for r in st.session_state.query_results)
        matched_remedial = sum(len(r.get("scholar_remedial", {})) for r in st.session_state.query_results)
        unmatched = sum(len(r["not_found"]) for r in st.session_state.query_results)

        header = StringIO()
        header.write(f"å ±å‘Šç”¢å‡ºæ™‚é–“ï¼š{report_time}\n\n")
        header.write("èªªæ˜ï¼š\n")
        header.write("ç‚ºç¯€çœæ ¸å°æ™‚é–“ï¼Œæœ¬ç³»çµ±åªæŸ¥å°æœ‰DOIç¢¼çš„æœŸåˆŠè«–æ–‡ã€‚ä¸”ä¸¦æœªæª¢æŸ¥æœŸåˆŠåç¨±ã€ä½œè€…ã€å·æœŸã€é ç¢¼ã€‚åªé‡å°ç¯‡åé€²è¡Œæ ¸å°ã€‚\n")
        header.write("æœ¬ç³»çµ±åªæ˜¯ç‚ºäº†æä¾›åˆæ­¥ç¯©é¸ï¼Œæ¯”å°å¾Œæ‡‰æ¥è‘—é€²è¡Œäººå·¥æ ¸å°ï¼Œä»»ä½•äººéƒ½ä¸æ‡‰è©²ä»¥æœ¬ç³»çµ±æ ¸å°çµæœä½œç‚ºä»»ä½•å­¸è¡“å€«ç†åˆ¤æ–·ä¹‹åŸºç¤ã€‚\n\n")

        csv_buffer = StringIO()
        csv_buffer.write(header.getvalue())
        if not export_data:
            st.warning("âš ï¸ æ²’æœ‰å¯åŒ¯å‡ºçš„æŸ¥æ ¸çµæœã€‚")
        else:
            df_export = pd.DataFrame(export_data, columns=["æª”æ¡ˆåç¨±", "åŸå§‹åƒè€ƒæ–‡ç»", "æŸ¥æ ¸çµæœ", "é€£çµ"])
            df_export.to_csv(csv_buffer, index=False)

        # çµ±è¨ˆæ‰€æœ‰æª”æ¡ˆçš„ç¸½æ•¸
        total_files = len(st.session_state.query_results)
        total_refs = sum(len(r["title_pairs"]) for r in st.session_state.query_results)
        matched_crossref = sum(len(r["crossref_doi_hits"]) for r in st.session_state.query_results)
        matched_scopus = sum(len(r["scopus_hits"]) for r in st.session_state.query_results)
        matched_scholar = sum(len(r["scholar_hits"]) for r in st.session_state.query_results)
        matched_remedial = sum(len(r.get("scholar_remedial", {})) for r in st.session_state.query_results)
        matched_similar = sum(len(r["scholar_similar"]) for r in st.session_state.query_results)
        matched_notfound = sum(len(r["not_found"]) for r in st.session_state.query_results)


        st.markdown(f"""
        ğŸ“Œ æŸ¥æ ¸çµæœèªªæ˜ï¼šæœ¬æ¬¡å…±è™•ç† **{total_files} ç¯‡è«–æ–‡**ï¼Œç¸½å…±æ“·å– **{total_refs} ç¯‡åƒè€ƒæ–‡ç»**ï¼Œå…¶ä¸­ï¼š

        - {matched_crossref} ç¯‡ç‚ºã€ŒCrossref æœ‰ DOI è³‡è¨Šã€
        - {matched_scopus} ç¯‡ç‚ºã€Œæ¨™é¡Œå‘½ä¸­ï¼ˆScopusï¼‰ã€
        - {matched_scholar} ç¯‡ç‚ºã€Œæ¨™é¡Œå‘½ä¸­ï¼ˆGoogle Scholarï¼‰ã€
        - {matched_remedial} ç¯‡ç‚ºã€ŒGoogle Scholar è£œæ•‘å‘½ä¸­ã€
        - {matched_similar} ç¯‡ç‚ºã€ŒGoogle Scholar é¡ä¼¼æ¨™é¡Œã€
        - {matched_notfound} ç¯‡ç‚ºã€ŒæŸ¥ç„¡çµæœã€
        """)
        st.markdown("---")
        
        st.subheader("ğŸ“¥ ä¸‹è¼‰æŸ¥è©¢çµæœ")

        st.download_button(
            label="ğŸ“¤ ä¸‹è¼‰çµæœ CSV æª”",
            data=csv_buffer.getvalue().encode('utf-8-sig'),
            file_name="reference_results.csv",
            mime="text/csv"
        )
        st.write("ğŸ” è‹¥è¦é‡æ–°ä¸Šå‚³æª”æ¡ˆï¼Œè«‹æŒ‰ä¸‹éµç›¤ä¸Šçš„ F5 æˆ–é»æ“Šç€è¦½å™¨é‡æ–°æ•´ç†æŒ‰éˆ•")    