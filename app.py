import streamlit as st
import re
import urllib.parse
from docx import Document
import tempfile
import requests
from difflib import SequenceMatcher
import pandas as pd
from datetime import datetime
from io import StringIO
from serpapi import GoogleSearch

# ========== API Key ç®¡ç† ==========
def get_scopus_key():
    try:
        return st.secrets["scopus_api_key"]
    except Exception:
        try:
            with open("scopus_key.txt", "r") as f:
                return f.read().strip()
        except FileNotFoundError:
            st.error("âŒ æ‰¾ä¸åˆ° Scopus API é‡‘é‘°ï¼Œè«‹ç¢ºèªå·²è¨­å®š secrets æˆ–æä¾› scopus_key.txt")
            st.stop()

SCOPUS_API_KEY = get_scopus_key()

def get_serpapi_key():
    try:
        return st.secrets["serpapi_key"]
    except Exception:
        try:
            with open("serpapi_key.txt", "r") as f:
                return f.read().strip()
        except FileNotFoundError:
            st.error("âŒ æ‰¾ä¸åˆ° SerpAPI é‡‘é‘°ï¼Œè«‹ç¢ºèªå·²è¨­å®š secrets æˆ–æä¾› serpapi_key.txt")
            st.stop()

SERPAPI_KEY = get_serpapi_key()

# ========== æ“·å– DOI ==========
def extract_doi(text):
    match = re.search(r'(10\.\d{4,9}/[-._;()/:A-Z0-9]+)', text, re.I)
    if match:
        return match.group(1).rstrip(".")

    doi_match = re.search(r'doi:\s*(https?://doi\.org/)?(10\.\d{4,9}/[-._;()/:A-Z0-9]+)', text, re.I)
    if doi_match:
        return doi_match.group(2).rstrip(".")

    return None

# ========== Crossref DOI æŸ¥è©¢ ==========
def search_crossref_by_doi(doi):
    url = f"https://api.crossref.org/works/{doi}"
    response = requests.get(url)
    if response.status_code == 200:
        item = response.json().get("message", {})
        titles = item.get("title")
        if isinstance(titles, list) and len(titles) > 0:
            return titles[0], item.get("URL")
        else:
            return None, item.get("URL")
    return None, None

# ========== æ¸…æ´—æ¨™é¡Œ ==========
def clean_title(text):
    text = text.lower().strip()
    text = re.sub(r'[â€œâ€â€˜â€™]', '"', text)
    text = re.sub(r'[:ï¼š]{2,}', ':', text)
    text = re.sub(r'[^a-z0-9\s:.,\\-]', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = text.rstrip('.,:;- ')
    return text

# ========== Scopus æŸ¥è©¢ ==========
def search_scopus_by_title(title):
    base_url = "https://api.elsevier.com/content/search/scopus"
    headers = {
        "Accept": "application/json",
        "X-ELS-APIKey": SCOPUS_API_KEY
    }
    params = {
        "query": f'TITLE("{title}")',
        "count": 1
    }
    response = requests.get(base_url, headers=headers, params=params)
    if response.status_code == 200:
        data = response.json()
        entries = data.get('search-results', {}).get('entry', [])
        for entry in entries:
            doc_title = entry.get('dc:title', '')
            if doc_title.strip().lower() == title.strip().lower():
                return entry.get('prism:url', 'https://www.scopus.com')
    return None

# ========== Serpapi æŸ¥è©¢ ==========
def search_scholar_by_title(title, api_key, threshold=0.90):
    search_url = f"https://scholar.google.com/scholar?q={urllib.parse.quote(title)}"

    # å‘¼å« SerpAPI
    params = {
        "engine": "google_scholar",
        "q": title,
        "api_key": api_key,
        "num": 1
    }
    results = GoogleSearch(params).get_dict()
    organic = results.get("organic_results", [])

    if not organic:
        return search_url, "no_result"
    
    cleaned_query = clean_title(title)
    
    for result in organic:
        result_title = result.get("title", "")
        cleaned_result = clean_title(result_title)
        
        # åƒ…ç•¶æ¨™é¡Œã€Œæ¸…æ´—å¾Œå®Œå…¨ä¸€è‡´ã€æ‰ç®— match
        if cleaned_query == cleaned_result:
            return search_url, "match"

        similarity = SequenceMatcher(None, cleaned_query, cleaned_result).ratio()
        if similarity >= threshold:
            return search_url, "similar"

    return search_url, "no_result"

# ========== Word è™•ç† ==========
def extract_paragraphs_from_docx(file):
    doc = Document(file)
    return [para.text.strip() for para in doc.paragraphs if para.text.strip()]

def extract_reference_section(paragraphs, start_keyword):
    start_index = -1
    for i, p in enumerate(paragraphs):
        if start_keyword.lower() in p.lower():
            start_index = i + 1
            break
    return paragraphs[start_index:] if start_index != -1 else []

# ========== æ“·å–æ¨™é¡Œ ==========
def extract_title(ref_text, style):
    if style == "APA":
        match = re.search(r'\(\d{4}\)\.\s(.+?)(?:\.\s|$)', ref_text)
        if match:
            return match.group(1).strip()
    elif style == "IEEE":
        matches = re.findall(r'"([^"]+)"', ref_text)
        if matches:
            return max(matches, key=len).strip().rstrip(",.")
        else:
            fallback = re.search(r'(?<!et al)([A-Z][^,.]+[a-zA-Z])[,\.]', ref_text)
            if fallback:
                return fallback.group(1).strip(" ,.")
    return None

# ========== Streamlit UI ==========
st.set_page_config(page_title="Reference Checker", layout="centered")
if "start_query" not in st.session_state:
    st.session_state.start_query = False
if "query_results" not in st.session_state:
    st.session_state.query_results = None
st.title("ğŸ“š Reference Checker")

st.markdown("""
<div style="background-color: #fff9db; padding: 15px; border-left: 6px solid #f1c40f; border-radius: 6px;">
    <span style="font-size: 16px; font-weight: bold;">âš ï¸ æ³¨æ„äº‹é …</span><br>
    <span style="font-size: 15px; color: #444;">
    ç‚ºç¯€çœæ ¸å°æ™‚é–“ï¼Œæœ¬ç³»çµ±åªæŸ¥å°æœ‰ DOI ç¢¼çš„æœŸåˆŠè«–æ–‡ã€‚ä¸¦æœªæª¢æŸ¥æœŸåˆŠåç¨±ã€ä½œè€…ã€å·æœŸã€é ç¢¼ï¼Œåƒ…é‡å°ç¯‡åé€²è¡Œæ ¸å°ã€‚æœ¬ç³»çµ±åƒ…æä¾›åˆæ­¥ç¯©é¸åƒè€ƒï¼Œæ¯”å°å¾Œæ‡‰é€²è¡Œäººå·¥æ ¸å°ï¼Œä¸å¾—ç›´æ¥ä»¥æœ¬ç³»çµ±æ ¸å°çµæœä½œç‚ºå­¸è¡“å€«ç†åˆ¤æ–·çš„ä¾æ“šã€‚
    </span>
</div>
""", unsafe_allow_html=True)
st.markdown(" ")
uploaded_file = st.file_uploader("è«‹ä¸Šå‚³ Word æª”æ¡ˆï¼ˆ.docxï¼‰", type=["docx"])
style = st.selectbox("è«‹é¸æ“‡åƒè€ƒæ–‡ç»æ ¼å¼", ["APA", "IEEE"])
#start_keyword = st.selectbox("è«‹é¸æ“‡åƒè€ƒæ–‡ç»èµ·å§‹æ¨™é¡Œ", ["åƒè€ƒæ–‡ç»", "References", "Reference"])
start_button = st.button("ğŸš€ é–‹å§‹æŸ¥è©¢")

# ========== ä¸Šå‚³ä¸¦è™•ç† ==========
if "selected_kw" not in st.session_state:
    st.session_state.selected_kw = None
if "paragraphs" not in st.session_state:
    st.session_state.paragraphs = None

if uploaded_file and start_button:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    paragraphs = extract_paragraphs_from_docx(tmp_path)
    st.session_state.paragraphs = paragraphs  # å„²å­˜ä¸‹ä¾†ä¾›å¾ŒçºŒä½¿ç”¨

    auto_keywords = ["åƒè€ƒæ–‡ç»", "References", "Reference"]
    matched_section = []
    matched_keyword = None

    for kw in auto_keywords:
        matched_section = extract_reference_section(paragraphs, kw)
        if matched_section:
            matched_keyword = kw
            st.session_state.selected_kw = matched_keyword
            st.session_state.matched_section = matched_section
            break

    else:
        st.warning("âš ï¸ ç„¡æ³•åµæ¸¬åƒè€ƒæ–‡ç»èµ·å§‹æ¨™é¡Œï¼Œè«‹ç¢ºèªæ˜¯å¦ç‚ºä»¥ä¸‹å…¶ä¸­ä¹‹ä¸€ï¼šã€åƒè€ƒæ–‡ç»ã€ã€ã€Referencesã€æˆ–ã€Referenceã€ã€‚")
        st.stop()

    if matched_section:
        title_pairs = []
        for ref in matched_section:
            title = extract_title(ref, style)
            if title:
                title_pairs.append((ref, title))

        #é–‹å§‹æŸ¥è©¢
        st.subheader("ğŸ“Š æ­£åœ¨æŸ¥è©¢ä¸­ï¼Œè«‹ç¨å€™...")
        crossref_doi_hits = {}
        scopus_hits = {}
        scholar_hits = {}
        scholar_similar = {}
        not_found = []

        progress_bar = st.progress(0.0)

        for i, (original_ref, title) in enumerate(title_pairs, 1):
            doi = extract_doi(original_ref)
            if doi:
                title_from_doi, url = search_crossref_by_doi(doi)
                if title_from_doi:
                    crossref_doi_hits[original_ref] = url
                    progress_bar.progress(i / len(title_pairs))
                    continue  # æˆåŠŸæŸ¥åˆ° DOI å°±ç•¥éæ¨™é¡ŒæŸ¥è©¢

            url = search_scopus_by_title(title)
            if url:
                scopus_hits[original_ref] = url
            else:
                gs_url, gs_type = search_scholar_by_title(title, SERPAPI_KEY)
                if gs_type == "match":
                    scholar_hits[original_ref] = gs_url
                elif gs_type == "similar":
                    scholar_similar[original_ref] = gs_url  # åŠ å…¥ similar åˆ†é¡
                else:
                    not_found.append(original_ref)

            progress_bar.progress(i / len(title_pairs))
            
        st.session_state.query_results = {
            "title_pairs": title_pairs,
            "crossref_doi_hits": crossref_doi_hits,
            "scopus_hits": scopus_hits,
            "scholar_hits": scholar_hits,
            "scholar_similar": scholar_similar,
            "not_found": not_found,
            "uploaded_filename": uploaded_file.name,
            "report_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
         # è¦å‰‡è¡¨æ ¼
        st.markdown("---")
        st.subheader("ğŸ§  æŸ¥è©¢çµæœåˆ†é¡è¦å‰‡")
        rules = [
            ["ğŸŸ¢ Crossref DOI å‘½ä¸­", "Crossref", "ä½¿ç”¨åƒè€ƒæ–‡ç»ä¸­çš„ DOI ç›´æ¥æŸ¥è©¢", "å¦"],
            ["ğŸŸ¢ æ¨™é¡Œå‘½ä¸­", "Scopus / Google Scholar", "æ¨™é¡Œå®Œå…¨ä¸€è‡´", "å¦"],
            ["ğŸŸ¡ Google Scholar é¡ä¼¼æ¨™é¡Œ", "Google Scholar", "æ¨™é¡Œç›¸ä¼¼æˆ–åƒ…éƒ¨åˆ†é—œéµå­—ä¸€è‡´", "æ˜¯"],
            ["ğŸ”´ æŸ¥ç„¡çµæœ", "â€”", "ç„¡ä»»ä½•çµæœæˆ–ç›¸ä¼¼åº¦éä½", "â€”"],
        ]
        df_rules = pd.DataFrame(rules, columns=["åˆ†é¡ç‡ˆè™Ÿ", "ä¾†æº", "æ¯”å°æ–¹å¼", "éœ€äººå·¥ç¢ºèª"])
        st.dataframe(df_rules, use_container_width=True)


if st.session_state.query_results:
        st.markdown("---")
        st.subheader("ğŸ“Š æŸ¥è©¢çµæœåˆ†é¡")

        query_data = st.session_state.query_results
        not_found = query_data.get("not_found", [])
        title_pairs = query_data["title_pairs"]
        crossref_doi_hits = query_data["crossref_doi_hits"]
        scholar_similar = query_data["scholar_similar"]
        uploaded_filename = query_data["uploaded_filename"]
        report_time = query_data["report_time"]

        scopus_hits = query_data["scopus_hits"]
        scholar_hits = query_data["scholar_hits"]

        matched_count = len(crossref_doi_hits) + len(scopus_hits) + len(scholar_hits)

        hit_tab, similar_tab, miss_tab = st.tabs([
            f"ğŸŸ¢ å‘½ä¸­çµæœï¼ˆ{matched_count}ï¼‰",
            f"ğŸŸ¡ Google Scholar é¡ä¼¼æ¨™é¡Œï¼ˆ{len(scholar_similar)}ï¼‰",
            f"ğŸ”´ å‡æŸ¥ç„¡çµæœï¼ˆ{len(not_found)}ï¼‰"
        ])

        with hit_tab:
            if crossref_doi_hits:
                with st.expander(f"\U0001F7E2 Crossref DOI å‘½ä¸­ï¼ˆ{len(crossref_doi_hits)}ï¼‰"):
                    for i, (title, url) in enumerate(crossref_doi_hits.items(), 1):
                        st.markdown(f"{i}. {title}  \nğŸ”— [DOI é€£çµ]({url})", unsafe_allow_html=True)

            if scopus_hits:
                with st.expander(f"\U0001F7E2 Scopus æ¨™é¡Œå‘½ä¸­ï¼ˆ{len(scopus_hits)}ï¼‰"):
                    for i, (title, url) in enumerate(scopus_hits.items(), 1):
                        st.markdown(f"{i}. {title}  \nğŸ”— [Scopus é€£çµ]({url})", unsafe_allow_html=True)

            if scholar_hits:
                with st.expander(f"\U0001F7E2 Google Scholar æ¨™é¡Œå‘½ä¸­ï¼ˆ{len(scholar_hits)}ï¼‰"):
                    for i, (title, url) in enumerate(scholar_hits.items(), 1):
                        st.markdown(f"{i}. {title}  \nğŸ”— [Scholar é€£çµ]({url})", unsafe_allow_html=True)

            if not (crossref_doi_hits or scopus_hits or scholar_hits):
                st.info("æ²’æœ‰å‘½ä¸­ä»»ä½•åƒè€ƒæ–‡ç»ã€‚")

        with similar_tab:
            if scholar_similar:
                for i, (title, url) in enumerate(scholar_similar.items(), 1):
                    with st.expander(f"{i}. {title}"):
                        st.markdown(f"ğŸ”— [Google Scholar çµæœé€£çµ]({url})", unsafe_allow_html=True)
                        st.warning("âš ï¸ æ­¤ç‚ºç›¸ä¼¼æ¨™é¡Œï¼Œè«‹äººå·¥ç¢ºèªæ˜¯å¦ç‚ºæ­£ç¢ºæ–‡ç»ã€‚")
            else:
                st.info("ç„¡æ¨™é¡Œç›¸ä¼¼ä½†ä¸ä¸€è‡´çš„çµæœã€‚")

        with miss_tab:
            if not_found:
                for i, title in enumerate(not_found, 1):
                    scholar_url = f"https://scholar.google.com/scholar?q={urllib.parse.quote(title)}"
                    st.markdown(f"{i}. {title}  \nğŸ”— [Google Scholar æœå°‹]({scholar_url})", unsafe_allow_html=True)
                st.markdown("ğŸ‘‰ è«‹è€ƒæ…®æ‰‹å‹•æœå°‹ Google Scholarã€‚")
            else:
                st.success("æ‰€æœ‰æ¨™é¡Œçš†æˆåŠŸæŸ¥è©¢ï¼")

        # ä¸‹è¼‰çµæœ
        st.markdown("---")
        export_data = []
        for ref, title in title_pairs:
            if ref in crossref_doi_hits:
                export_data.append([ref, "Crossref æœ‰ DOI è³‡è¨Š", crossref_doi_hits[ref]])
            elif ref in scopus_hits:
                export_data.append([ref, "æ¨™é¡Œå‘½ä¸­ï¼ˆScopusï¼‰", scopus_hits[ref]])
            elif ref in scholar_hits:
                export_data.append([ref, "æ¨™é¡Œå‘½ä¸­ï¼ˆGoogle Scholarï¼‰", scholar_hits[ref]])
            elif ref in scholar_similar:
                export_data.append([ref, "Google Scholar é¡ä¼¼æ¨™é¡Œ", scholar_similar[ref]])
            elif ref in not_found:
                scholar_url = f"https://scholar.google.com/scholar?q={urllib.parse.quote(ref)}"
                export_data.append([ref, "æŸ¥ç„¡çµæœ", scholar_url])

        total_refs = len(title_pairs)
        matched_exact = len(crossref_doi_hits) + len(scopus_hits) + len(scholar_hits)
        matched_similar = len(scholar_similar)
        unmatched = len(not_found)

        header = StringIO()
        header.write(f"æª”æ¡ˆåç¨±ï¼š{uploaded_filename}\n")
        header.write(f"å ±å‘Šç”¢å‡ºæ™‚é–“ï¼š{report_time}\n\n")
        header.write("åˆæ­¥ç¯©é¸æ ¸å°çµæœï¼š\n")
        header.write(f"æœ¬ç¯‡è«–æ–‡å…±æœ‰ {total_refs} ç¯‡åƒè€ƒæ–‡ç»ï¼Œå…¶ä¸­æœ‰ {matched_exact} ç¯‡æœ‰æ‰¾åˆ°ç›¸åŒç¯‡åï¼Œæœ‰ {matched_similar} ç¯‡æ‰¾åˆ°é¡ä¼¼ç¯‡åï¼Œ{unmatched} ç¯‡æœªæ‰¾åˆ°å°æ‡‰çš„æœŸåˆŠè«–æ–‡ï¼Œå¯èƒ½æ˜¯å°ˆæ›¸ã€ç ”è¨æœƒè«–æ–‡ã€ç”¢æ¥­å ±å‘Šæˆ–å…¶ä»–è«–æ–‡ï¼Œéœ€è¦äººå·¥é€²è¡Œå¾ŒçºŒæ ¸å°ã€‚\n\n")
        header.write("èªªæ˜ï¼š\n")
        header.write("ç‚ºç¯€çœæ ¸å°æ™‚é–“ï¼Œæœ¬ç³»çµ±åªæŸ¥å°æœ‰DOIç¢¼çš„æœŸåˆŠè«–æ–‡ã€‚ä¸”ä¸¦æœªæª¢æŸ¥æœŸåˆŠåç¨±ã€ä½œè€…ã€å·æœŸã€é ç¢¼ã€‚åªé‡å°ç¯‡åé€²è¡Œæ ¸å°ã€‚\n")
        header.write("æœ¬ç³»çµ±åªæ˜¯ç‚ºäº†æä¾›åˆæ­¥ç¯©é¸ï¼Œæ¯”å°å¾Œæ‡‰æ¥è‘—é€²è¡Œäººå·¥æ ¸å°ï¼Œä»»ä½•äººéƒ½ä¸æ‡‰è©²ä»¥æœ¬ç³»çµ±æ ¸å°çµæœä½œç‚ºä»»ä½•å­¸è¡“å€«ç†åˆ¤æ–·ä¹‹åŸºç¤ã€‚\n\n")

        csv_buffer = StringIO()
        csv_buffer.write(header.getvalue())
        df_export = pd.DataFrame(export_data, columns=["åŸå§‹åƒè€ƒæ–‡ç»", "æŸ¥æ ¸çµæœ", "é€£çµ"])
        df_export.to_csv(csv_buffer, index=False)

        st.markdown(f"""
        ğŸ“Œ æŸ¥æ ¸çµæœèªªæ˜ï¼šæœ¬ç¯‡è«–æ–‡å…±æœ‰ {total_refs} ç¯‡åƒè€ƒæ–‡ç»ï¼Œå…¶ä¸­ï¼š

        - {len(crossref_doi_hits)} ç¯‡ç‚ºã€ŒCrossref æœ‰ DOI è³‡è¨Šã€
        - {len(scopus_hits)} ç¯‡ç‚ºã€Œæ¨™é¡Œå‘½ä¸­ï¼ˆScopusï¼‰ã€
        - {len(scholar_hits)} ç¯‡ç‚ºã€Œæ¨™é¡Œå‘½ä¸­ï¼ˆGoogle Scholarï¼‰ã€
        - {len(scholar_similar)} ç¯‡ç‚ºã€ŒGoogle Scholar é¡ä¼¼æ¨™é¡Œã€
        - {len(not_found)} ç¯‡ç‚ºã€ŒæŸ¥ç„¡çµæœã€
        """)
        st.markdown("---")
        
        st.subheader("ğŸ“¥ ä¸‹è¼‰æŸ¥è©¢çµæœ")
        col1, col2 = st.columns([1, 1])
        with col1:
            st.download_button(
                label="ğŸ“¤ ä¸‹è¼‰çµæœ CSV æª”",
                data=csv_buffer.getvalue().encode('utf-8-sig'),
                file_name="reference_results.csv",
                mime="text/csv"
            )
        with col2:
            if st.button("ğŸ” é‡æ–°ä¸Šå‚³å…¶ä»–æª”æ¡ˆ"):
                for key in list(st.session_state.keys()):
                    del st.session_state[key]
                st.rerun()