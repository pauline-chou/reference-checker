import streamlit as st
import re
import urllib.parse
from docx import Document
import tempfile
import requests
from difflib import SequenceMatcher
import pandas as pd
from datetime import datetime
from io import StringIO

# ========== API Key ç®¡ç† ==========
def get_scopus_key():
    try:
        return st.secrets["scopus_api_key"]
    except Exception:
        try:
            with open("scopus_key.txt", "r") as f:
                return f.read().strip()
        except FileNotFoundError:
            st.error("âŒ æ‰¾ä¸åˆ° Scopus API é‡‘é‘°ï¼Œè«‹ç¢ºèªå·²è¨­å®š secrets æˆ–æä¾› scopus_key.txt")
            st.stop()

SCOPUS_API_KEY = get_scopus_key()

# ========== æ¸…æ´—æ¨™é¡Œ ==========
def clean_title(text):
    # å»é™¤æ¨™é»ã€ç©ºç™½ï¼Œä¸¦è½‰ç‚ºå°å¯«
    return re.sub(r'\W+', '', text).lower()

# ========== ç›¸ä¼¼åº¦åˆ¤æ–· ==========
def is_similar(a, b, threshold=0.9):
    return SequenceMatcher(None, a, b).ratio() >= threshold

# ========== Crossref æŸ¥è©¢ ==========
def search_crossref_by_title(title):
    crossref_email = st.secrets.get("crossref_email", "your_email@example.com")  # â† ä½¿ç”¨è€…è¨˜å¾—è‡ªè¡Œä¿®æ”¹
    url = "https://api.crossref.org/works"
    params = {
        "query.title": title,
        "rows": 5,
        "mailto": crossref_email
    }

    response = requests.get(url, params=params)
    if response.status_code != 200:
        return (None, None)

    items = response.json().get("message", {}).get("items", [])
    cleaned_input = clean_title(title)

    for item in items:
        cr_title = item.get("title", [""])[0]
        cr_url = item.get("URL")
        cleaned_cr_title = clean_title(cr_title)

        if cleaned_input == cleaned_cr_title:
            return ("exact", cr_url)
        elif is_similar(cleaned_input, cleaned_cr_title, threshold=0.9):
            return ("similar", cr_url)

    return (None, None)

# ========== Scopus æŸ¥è©¢ ==========
def search_scopus_by_title(title):
    base_url = "https://api.elsevier.com/content/search/scopus"
    headers = {
        "Accept": "application/json",
        "X-ELS-APIKey": SCOPUS_API_KEY
    }
    params = {
        "query": f'TITLE("{title}")',
        "count": 5
    }
    response = requests.get(base_url, headers=headers, params=params)
    if response.status_code == 200:
        data = response.json()
        entries = data.get('search-results', {}).get('entry', [])
        for entry in entries:
            doc_title = entry.get('dc:title', '')
            if doc_title.strip().lower() == title.strip().lower():
                return entry.get('prism:url', 'https://www.scopus.com')
    return None

# ========== Word è™•ç† ==========
def extract_paragraphs_from_docx(file):
    doc = Document(file)
    return [para.text.strip() for para in doc.paragraphs if para.text.strip()]

def extract_reference_section(paragraphs, start_keyword):
    start_index = -1
    for i, p in enumerate(paragraphs):
        if start_keyword.lower() in p.lower():
            start_index = i + 1
            break
    return paragraphs[start_index:] if start_index != -1 else []

# ========== æ“·å–æ¨™é¡Œ ==========
def extract_title(ref_text, style):
    if style == "APA":
        match = re.search(r'\(\d{4}\)\.\s(.+?)(\.|\n|$)', ref_text)
        if match:
            return match.group(1).strip()
    elif style == "IEEE":
        match = re.search(r'"(.+?)"', ref_text)
        if match:
            return match.group(1).strip()
    return None

# ========== Streamlit UI ==========
st.set_page_config(page_title="Reference Checker", layout="centered")
st.title("ğŸ“š Reference Checker")
st.write("ä¸Šå‚³ Word æª” (.docx)ï¼Œè‡ªå‹•æŸ¥è©¢ Scopus â†’ Crossrefï¼Œåˆ†é¡ç‚ºå››é¡")

uploaded_file = st.file_uploader("è«‹ä¸Šå‚³ Word æª”æ¡ˆï¼ˆ.docxï¼‰", type=["docx"])
style = st.selectbox("è«‹é¸æ“‡åƒè€ƒæ–‡ç»æ ¼å¼", ["APA", "IEEE"])
#start_keyword = st.selectbox("è«‹é¸æ“‡åƒè€ƒæ–‡ç»èµ·å§‹æ¨™é¡Œ", ["åƒè€ƒæ–‡ç»", "References", "Reference"])
start_button = st.button("ğŸš€ é–‹å§‹æŸ¥è©¢")
# ========== ä¸Šå‚³ä¸¦è™•ç† ==========
if uploaded_file and start_button:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

        # âœ… å…ˆè§£ææ•´ä»½ Word æ®µè½
        paragraphs = extract_paragraphs_from_docx(tmp_path)

        # âœ… å˜—è©¦ä½¿ç”¨é è¨­é—œéµå­—æ“·å–åƒè€ƒæ–‡ç»æ®µè½
        auto_keywords = ["åƒè€ƒæ–‡ç»", "References", "Reference"]
        matched_section = []
        matched_keyword = None

        for kw in auto_keywords:
            matched_section = extract_reference_section(paragraphs, kw)
            if matched_section:
                matched_keyword = kw
                break

        # âœ… è‹¥æ‰¾ä¸åˆ° â†’ é¡¯ç¤ºæ‰‹å‹•è¼¸å…¥æ¬„ä½
        if not matched_section:
            st.warning("âš ï¸ ç„¡æ³•è‡ªå‹•åµæ¸¬åƒè€ƒæ–‡ç»æ¨™é¡Œï¼Œè«‹æ‰‹å‹•è¼¸å…¥ï¼š")
            manual_kw = st.text_input("è«‹è¼¸å…¥ Word ä¸­åƒè€ƒæ–‡ç»æ¨™é¡Œï¼ˆä¾‹å¦‚ åƒè€ƒæ–‡ç» / Reference / Works Citedï¼‰")
            if manual_kw:
                matched_section = extract_reference_section(paragraphs, manual_kw)
                if not matched_section:
                    st.error("âŒ ä»ç„¶ç„¡æ³•æ‰¾åˆ°åƒè€ƒæ–‡ç»æ®µè½ï¼Œè«‹ç¢ºèªè¼¸å…¥å…§å®¹æˆ–æª”æ¡ˆæ ¼å¼ã€‚")
                else:
                    matched_keyword = manual_kw

        references = matched_section

        if not references:
            st.warning("âš ï¸ æ‰¾ä¸åˆ°åƒè€ƒæ–‡ç»æ®µè½ï¼Œè«‹ç¢ºèªæª”æ¡ˆæ ¼å¼èˆ‡æ¨™é¡Œã€‚")
        else:
            title_pairs = []
            for ref in references:
                title = extract_title(ref, style)
                if title:
                    title_pairs.append((ref, title))  # åŸå§‹å­—ä¸²èˆ‡æ¨™é¡Œé…å°

            # âœ… è¦å‰‡è¡¨æ ¼ï¼šæå‰é¡¯ç¤º
            st.markdown("---")
            st.subheader("ğŸ§  æŸ¥è©¢çµæœåˆ†é¡è¦å‰‡")
            rules = [
                ["ğŸŸ¢ Scopus é¦–æ¬¡æ‰¾åˆ°", "Scopus", "æ¨™é¡Œå®Œå…¨ä¸€è‡´", "å¦"],
                ["ğŸŸ¢ Crossref å®Œå…¨åŒ…å«", "Crossref", "æŸ¥è©¢æ¨™é¡ŒåŒ…å«æ–¼ Crossref æ¨™é¡Œä¸­", "å¦"],
                ["ğŸŸ¡ Crossref é¡ä¼¼æ¨™é¡Œ", "Crossref", "æ¨™é¡Œç›¸ä¼¼åº¦ â‰¥ 0.9", "æ˜¯"],
                ["ğŸ”´ å‡æŸ¥ç„¡çµæœ", "â€”", "ç„¡ä»»ä½•çµæœæˆ–ç›¸ä¼¼åº¦éä½", "â€”"],
            ]
            df_rules = pd.DataFrame(rules, columns=["åˆ†é¡ç‡ˆè™Ÿ", "ä¾†æº", "æ¯”å°æ–¹å¼", "éœ€äººå·¥ç¢ºèª"])
            st.dataframe(df_rules, use_container_width=True)

            # âœ… çµæœå€é ç•™
            result_tabs_placeholder = st.empty()

            # âœ… é–‹å§‹æŸ¥è©¢
            st.subheader("ğŸ“Š æ­£åœ¨æŸ¥è©¢ä¸­ï¼Œè«‹ç¨å€™...")
            scopus_results = {}
            crossref_exact = {}
            crossref_similar = {}
            not_found = []

            progress_bar = st.progress(0.0)

        for i, (original_ref, title) in enumerate(title_pairs, 1):
            msg_box = st.empty()
            with st.status(f"ğŸ” ç¬¬ {i} ç­†ï¼š`{title}`", expanded=True) as status:
                msg_box.markdown("ğŸ“¡ æ­£åœ¨æŸ¥ Scopus...")
                url = search_scopus_by_title(title)
                if url:
                    scopus_results[original_ref] = url
                    msg_box.markdown("âœ… å·²æ‰¾åˆ°æ–¼ **Scopus**")
                    status.update(label=f"ğŸŸ¢ ç¬¬ {i} ç­†æˆåŠŸï¼ˆScopusï¼‰", state="complete")
                else:
                    msg_box.markdown("ğŸ” Scopus ç„¡çµæœï¼Œæ”¹æŸ¥ Crossref...")
                    match_type, url = search_crossref_by_title(title)
                    if match_type == "exact":
                        crossref_exact[original_ref] = url
                        msg_box.markdown("âœ… Crossref å®Œå…¨åŒ…å«")
                        status.update(label=f"ğŸŸ¢ ç¬¬ {i} ç­†æˆåŠŸï¼ˆCrossref å®Œå…¨åŒ…å«ï¼‰", state="complete")
                    elif match_type == "similar":
                        crossref_similar[original_ref] = url
                        msg_box.markdown("ğŸŸ¡ Crossref æ¨™é¡Œç›¸ä¼¼ï¼ˆå»ºè­°äººå·¥ç¢ºèªï¼‰")
                        status.update(label=f"ğŸŸ¡ ç¬¬ {i} ç­†ç›¸ä¼¼ï¼ˆéœ€ç¢ºèªï¼‰", state="complete")
                    else:
                        not_found.append(original_ref)
                        msg_box.markdown("âŒ Crossref ä¹Ÿç„¡çµæœ")
                        status.update(label=f"ğŸ”´ ç¬¬ {i} ç­†æœªæ‰¾åˆ°", state="error")
            progress_bar.progress(i / len(title_pairs))

        # âœ… å°‡çµæœå¡«å…¥é ç•™å€å¡Š
        with result_tabs_placeholder.container():
            st.markdown("---")
            st.subheader("ğŸ“Š æŸ¥è©¢çµæœåˆ†é¡")

            tab1, tab2, tab3, tab4 = st.tabs([
                f"ğŸŸ¢ Scopus é¦–æ¬¡æ‰¾åˆ°ï¼ˆ{len(scopus_results)}ï¼‰",
                f"ğŸŸ¢ Crossref å®Œå…¨åŒ…å«ï¼ˆ{len(crossref_exact)}ï¼‰",
                f"ğŸŸ¡ Crossref é¡ä¼¼æ¨™é¡Œï¼ˆ{len(crossref_similar)}ï¼‰",
                f"ğŸ”´ å‡æŸ¥ç„¡çµæœï¼ˆ{len(not_found)}ï¼‰"
            ])

            with tab1:
                if scopus_results:
                    for i, (title, url) in enumerate(scopus_results.items(), 1):
                        with st.expander(f"{i}. {title}"):
                            st.markdown(f"ğŸ”— [Scopus é€£çµ]({url})", unsafe_allow_html=True)
                else:
                    st.info("Scopus ç„¡ä»»ä½•å‘½ä¸­çµæœã€‚")

            with tab2:
                if crossref_exact:
                    for i, (title, url) in enumerate(crossref_exact.items(), 1):
                        with st.expander(f"{i}. {title}"):
                            st.markdown(f"ğŸ”— [Crossref é€£çµ]({url})", unsafe_allow_html=True)
                else:
                    st.info("Crossref ç„¡å®Œå…¨åŒ…å«çµæœã€‚")

            with tab3:
                if crossref_similar:
                    for i, (title, url) in enumerate(crossref_similar.items(), 1):
                        with st.expander(f"{i}. {title}"):
                            st.markdown(f"ğŸ”— [ç›¸ä¼¼è«–æ–‡é€£çµ]({url})", unsafe_allow_html=True)
                            st.warning("âš ï¸ æ­¤ç‚ºç›¸ä¼¼æ¨™é¡Œï¼Œè«‹äººå·¥ç¢ºèªæ˜¯å¦ç‚ºæ­£ç¢ºæ–‡ç»ã€‚")
                else:
                    st.info("ç„¡æ¨™é¡Œç›¸ä¼¼ä½†ä¸ä¸€è‡´çš„çµæœã€‚")

            with tab4:
                if not_found:
                    for i, title in enumerate(not_found, 1):
                        scholar_url = f"https://scholar.google.com/scholar?q={urllib.parse.quote(title)}"
                        st.markdown(f"{i}. {title}  \nğŸ”— [Google Scholar æœå°‹]({scholar_url})", unsafe_allow_html=True)
                    st.markdown("ğŸ‘‰ è«‹è€ƒæ…®æ‰‹å‹•æœå°‹ Google Scholarã€‚")
                else:
                    st.success("æ‰€æœ‰æ¨™é¡Œçš†æˆåŠŸæŸ¥è©¢ï¼")
         # âœ… åŒ¯å‡º CSV æª”æ¡ˆ
            st.markdown("---")
            st.subheader("ğŸ“¥ ä¸‹è¼‰æŸ¥è©¢çµæœ")

            export_data = []
            for ref, title in title_pairs:
                if ref in scopus_results:
                    export_data.append([ref, "Scopus é¦–æ¬¡æ‰¾åˆ°", scopus_results[ref]])
                elif ref in crossref_exact:
                    export_data.append([ref, "Crossref å®Œå…¨åŒ…å«", crossref_exact[ref]])
                elif ref in crossref_similar:
                    export_data.append([ref, "Crossref é¡ä¼¼æ¨™é¡Œ", crossref_similar[ref]])
                elif ref in not_found:
                    scholar_url = f"https://scholar.google.com/scholar?q={urllib.parse.quote(ref)}"
                    export_data.append([ref, "æŸ¥ç„¡çµæœ", scholar_url])

            # çµ±è¨ˆæ•¸æ“š
            total_refs = len(title_pairs)
            matched_exact = len(scopus_results) + len(crossref_exact)
            matched_similar = len(crossref_similar)
            unmatched = len(not_found)

            # æª”åèˆ‡æ™‚é–“
            uploaded_filename = uploaded_file.name
            report_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")


            # å»ºç«‹ä¸» DataFrame
            df_export = pd.DataFrame(export_data, columns=["åŸå§‹åƒè€ƒæ–‡ç»", "åˆ†é¡", "é€£çµ"])

            # å°‡èªªæ˜èˆ‡çµ±è¨ˆæ’å…¥ç‚ºå‰æ®µæ–‡å­—ï¼ˆç”¨ StringIO ä¸²æ¥ï¼‰
            header = StringIO()
            header.write(f"æª”æ¡ˆåç¨±ï¼š{uploaded_filename}\n")
            header.write(f"å ±å‘Šç”¢å‡ºæ™‚é–“ï¼š{report_time}\n\n")
            header.write("åˆæ­¥ç¯©é¸æ ¸å°çµæœï¼š\n")
            header.write(f"æœ¬ç¯‡è«–æ–‡å…±æœ‰ {total_refs} ç¯‡åƒè€ƒæ–‡ç»ï¼Œå…¶ä¸­æœ‰ {matched_exact} ç¯‡æœ‰æ‰¾åˆ°ç›¸åŒç¯‡åï¼Œæœ‰ {matched_similar} ç¯‡æ‰¾åˆ°é¡ä¼¼ç¯‡åï¼Œ{unmatched} ç¯‡æœªæ‰¾åˆ°å°æ‡‰çš„æœŸåˆŠè«–æ–‡ï¼Œå¯èƒ½æ˜¯å°ˆæ›¸ã€ç ”è¨æœƒè«–æ–‡ã€ç”¢æ¥­å ±å‘Šæˆ–å…¶ä»–è«–æ–‡ï¼Œéœ€è¦äººå·¥é€²è¡Œå¾ŒçºŒæ ¸å°ã€‚\n\n")
            header.write("èªªæ˜ï¼š\n")
            header.write("ç‚ºç¯€çœæ ¸å°æ™‚é–“ï¼Œæœ¬ç³»çµ±åªæŸ¥å°æœ‰DOIç¢¼çš„æœŸåˆŠè«–æ–‡ã€‚ä¸”ä¸¦æœªæª¢æŸ¥æœŸåˆŠåç¨±ã€ä½œè€…ã€å·æœŸã€é ç¢¼ã€‚åªé‡å°ç¯‡åé€²è¡Œæ ¸å°ã€‚\n")
            header.write("æœ¬ç³»çµ±åªæ˜¯ç‚ºäº†æä¾›åˆæ­¥ç¯©é¸ï¼Œæ¯”å°å¾Œæ‡‰æ¥è‘—é€²è¡Œäººå·¥æ ¸å°ï¼Œä»»ä½•äººéƒ½ä¸æ‡‰è©²ä»¥æœ¬ç³»çµ±æ ¸å°çµæœä½œç‚ºä»»ä½•å­¸è¡“å€«ç†åˆ¤æ–·ä¹‹åŸºç¤ã€‚\n\n")

            # å¯«å…¥ä¸»è³‡æ–™
            csv_buffer = StringIO()
            header_content = header.getvalue()
            csv_buffer.write(header_content)
            df_export.to_csv(csv_buffer, index=False)

            st.download_button(
                label="ğŸ“¤ ä¸‹è¼‰çµæœ CSV æª”",
                data=csv_buffer.getvalue().encode('utf-8-sig'),
                file_name="reference_results.csv",
                mime="text/csv"
            )