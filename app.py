import streamlit as st
import re
import urllib.parse
from docx import Document
import tempfile
import requests

# ğŸ”‘ API Key ç®¡ç†ï¼šæ”¯æ´æœ¬æ©Ÿ .txt èˆ‡é›²ç«¯ secrets.toml
def get_scopus_key():
    if "scopus_api_key" in st.secrets:
        return st.secrets["scopus_api_key"]
    else:
        with open("scopus_key.txt", "r") as f:
            return f.read().strip()

SCOPUS_API_KEY = get_scopus_key()

# Streamlit è¨­å®š
st.set_page_config(page_title="Reference Checker", layout="centered")
st.title("ğŸ“š Reference Checker")
st.write("ä¸Šå‚³ Word æª” (.docx)ï¼Œç³»çµ±å°‡å¾åƒè€ƒæ–‡ç»å€é–‹å§‹æ“·å–å¼•ç”¨ï¼Œä¸¦å˜—è©¦ä»¥æ¨™é¡Œæœå°‹ Scopus æ–‡ç»ã€‚")

# ä¸Šå‚³æª”æ¡ˆèˆ‡é¸é …
uploaded_file = st.file_uploader("è«‹ä¸Šå‚³ Word æª”æ¡ˆï¼ˆ.docxï¼‰", type=["docx"])
style = st.selectbox("è«‹é¸æ“‡åƒè€ƒæ–‡ç»æ ¼å¼", ["APA", "IEEE"])
start_keyword = st.text_input("è«‹è¼¸å…¥åƒè€ƒæ–‡ç»èµ·å§‹æ¨™é¡Œï¼ˆä¾‹å¦‚ References æˆ– åƒè€ƒæ–‡ç»ï¼‰", "References")

# æ“·å– Word ä¸­æ‰€æœ‰æ®µè½
def extract_paragraphs_from_docx(file):
    doc = Document(file)
    return [para.text.strip() for para in doc.paragraphs if para.text.strip()]

# æ“·å–åƒè€ƒæ–‡ç»å€æ®µ
def extract_reference_section(paragraphs, start_keyword):
    start_index = -1
    for i, p in enumerate(paragraphs):
        if start_keyword.lower() in p.lower():
            start_index = i + 1
            break
    if start_index == -1:
        return []
    return paragraphs[start_index:]

# æ ¹æ“šæ ¼å¼æ“·å–æ¨™é¡Œ
def extract_title(ref_text, style):
    if style == "APA":
        match = re.search(r'\(\d{4}\)\.\s(.+?)(\.|\n|$)', ref_text)
        if match:
            return match.group(1).strip()
    elif style == "IEEE":
        match = re.search(r'"(.+?)"', ref_text)
        if match:
            return match.group(1).strip()
    return None

# ä½¿ç”¨ Scopus API æŸ¥è©¢æ¨™é¡Œ
def search_scopus_by_title(title):
    base_url = "https://api.elsevier.com/content/search/scopus"
    headers = {
        "Accept": "application/json",
        "X-ELS-APIKey": SCOPUS_API_KEY
    }
    params = {
        "query": f'TITLE("{title}")',
        "count": 5
    }

    response = requests.get(base_url, headers=headers, params=params)
    if response.status_code == 200:
        data = response.json()
        entries = data.get('search-results', {}).get('entry', [])
        for entry in entries:
            doc_title = entry.get('dc:title', '')
            if doc_title.strip().lower() == title.strip().lower():
                return entry.get('prism:url', 'https://www.scopus.com')
    return None

# ä¸»è™•ç†æµç¨‹
if uploaded_file:
    with tempfile.NamedTemporaryFile(delete=False) as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    paragraphs = extract_paragraphs_from_docx(tmp_path)
    references = extract_reference_section(paragraphs, start_keyword)

    if not references:
        st.warning("âš ï¸ æ‰¾ä¸åˆ°åƒè€ƒæ–‡ç»æ®µè½ï¼Œè«‹æª¢æŸ¥ã€åƒè€ƒæ–‡ç»æ¨™é¡Œé—œéµå­—ã€æ˜¯å¦æ­£ç¢ºã€‚")
    else:
        st.subheader("ğŸ” æŸ¥è©¢çµæœ")
        for i, ref in enumerate(references):
            title = extract_title(ref, style)
            if title:
                scopus_url = search_scopus_by_title(title)
                if scopus_url:
                    st.markdown(f"**{i+1}. {title}**  \nğŸ”— [Scopus æŸ¥è©¢çµæœ]({scopus_url})", unsafe_allow_html=True)
                else:
                    st.error(f"âš ï¸ ç¬¬ {i+1} ç­†æ‰¾ä¸åˆ°å®Œå…¨å»åˆçš„ Scopus æ–‡ç»ï¼š\n> {title}")
            else:
                st.error(f"âŒ ç¬¬ {i+1} ç­†ç„¡æ³•å¾ä¸­è§£ææ¨™é¡Œï¼š\n> {ref}")
