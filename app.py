import streamlit as st
import re
import urllib.parse
from docx import Document
import requests
from difflib import SequenceMatcher
import pandas as pd
from datetime import datetime
from io import StringIO
from serpapi import GoogleSearch
import fitz 
import re


# ========== API Key ç®¡ç† ==========
def get_scopus_key():
    try:
        return st.secrets["scopus_api_key"]
    except Exception:
        try:
            with open("scopus_key.txt", "r") as f:
                return f.read().strip()
        except FileNotFoundError:
            st.error("âŒ æ‰¾ä¸åˆ° Scopus API é‡‘é‘°ï¼Œè«‹ç¢ºèªå·²è¨­å®š secrets æˆ–æä¾› scopus_key.txt")
            st.stop()

SCOPUS_API_KEY = get_scopus_key()

def get_serpapi_key():
    try:
        return st.secrets["serpapi_key"]
    except Exception:
        try:
            with open("serpapi_key.txt", "r") as f:
                return f.read().strip()
        except FileNotFoundError:
            st.error("âŒ æ‰¾ä¸åˆ° SerpAPI é‡‘é‘°ï¼Œè«‹ç¢ºèªå·²è¨­å®š secrets æˆ–æä¾› serpapi_key.txt")
            st.stop()

SERPAPI_KEY = get_serpapi_key()

# ========== æ“·å– DOI ==========
def extract_doi(text):
    match = re.search(r'(10\.\d{4,9}/[-._;()/:A-Z0-9]+)', text, re.I)
    if match:
        return match.group(1).rstrip(".")

    doi_match = re.search(r'doi:\s*(https?://doi\.org/)?(10\.\d{4,9}/[-._;()/:A-Z0-9]+)', text, re.I)
    if doi_match:
        return doi_match.group(2).rstrip(".")

    return None

# ========== Crossref DOI æŸ¥è©¢ ==========
def search_crossref_by_doi(doi):
    url = f"https://api.crossref.org/works/{doi}"
    response = requests.get(url)
    if response.status_code == 200:
        item = response.json().get("message", {})
        titles = item.get("title")
        if isinstance(titles, list) and len(titles) > 0:
            return titles[0], item.get("URL")
        else:
            return None, item.get("URL")
    return None, None

# ========== æ¸…æ´—æ¨™é¡Œ ==========
def clean_title(text):
    text = text.lower().strip()
    text = re.sub(r'[â€œâ€â€˜â€™]', '"', text)
    text = re.sub(r'[:ï¼š]{2,}', ':', text)
    text = re.sub(r'[^a-z0-9\s:.,\\-]', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = text.rstrip('.,:;- ')
    return text

# ========== Scopus æŸ¥è©¢ ==========
def search_scopus_by_title(title):
    base_url = "https://api.elsevier.com/content/search/scopus"
    headers = {
        "Accept": "application/json",
        "X-ELS-APIKey": SCOPUS_API_KEY
    }
    params = {
        "query": f'TITLE("{title}")',
        "count": 3
    }
    response = requests.get(base_url, headers=headers, params=params)
    if response.status_code == 200:
        data = response.json()
        entries = data.get('search-results', {}).get('entry', [])
        for entry in entries:
            doc_title = entry.get('dc:title', '')
            if doc_title.strip().lower() == title.strip().lower():
                return entry.get('prism:url', 'https://www.scopus.com')
    return None

# ========== Serpapi æŸ¥è©¢ ==========
def search_scholar_by_title(title, api_key, threshold=0.90):
    search_url = f"https://scholar.google.com/scholar?q={urllib.parse.quote(title)}"
    params = {
        "engine": "google_scholar",
        "q": title,
        "api_key": api_key,
        "num": 3
    }

    try:
        results = GoogleSearch(params).get_dict()

        if "error" in results:
            error_msg = results["error"]
            st.session_state["serpapi_error"] = error_msg
            if any(keyword in error_msg.lower() for keyword in ["exceed", "limit", "run out", "searches"]):
                st.session_state["serpapi_exceeded"] = True
            return search_url, "no_result"


        organic = results.get("organic_results", [])
        if not organic:
            return search_url, "no_result"

        cleaned_query = clean_title(title)
        for result in organic:
            result_title = result.get("title", "")
            cleaned_result = clean_title(result_title)
            if cleaned_query == cleaned_result:
                return search_url, "match"
            if SequenceMatcher(None, cleaned_query, cleaned_result).ratio() >= threshold:
                return search_url, "similar"

        return search_url, "no_result"

    except Exception as e:
        st.session_state["serpapi_error"] = f"API æŸ¥è©¢éŒ¯èª¤ï¼š{e}"
        return search_url, "no_result"

    
# ========== Word è™•ç† ==========
def extract_paragraphs_from_docx(file):
    # ä½¿ç”¨ BytesIO è™•ç† UploadedFile
    doc = Document(file)
    return [para.text.strip() for para in doc.paragraphs if para.text.strip()]

# ========== PDF è™•ç† ==========
def extract_paragraphs_from_pdf(file):
    text = ""
    with fitz.open(stream=file.read(), filetype="pdf") as doc:
        for page in doc:
            page_text = page.get_text("text")
            text += page_text + "\n"
    paragraphs = [p.strip() for p in text.split("\n") if p.strip()]
    return paragraphs

# ========== èƒå–åƒè€ƒæ–‡ç» ==========
def extract_reference_section_from_bottom(paragraphs, start_keywords=None):
    """
    å¾åº•éƒ¨å¾€ä¸Šæ‰¾å‡ºçœŸæ­£çš„åƒè€ƒæ–‡ç»å€æ®µèµ·é»ï¼Œä¸¦å›å‚³é—œéµå­—ä¾†æº
    å›å‚³æ ¼å¼ï¼šmatched_section, matched_keyword
    """
    if start_keywords is None:
        start_keywords = [
            "åƒè€ƒæ–‡ç»", "references", "reference",
            "bibliography", "works cited", "literature cited"
        ]

    for i in reversed(range(len(paragraphs))):
        para = paragraphs[i].strip()

        # è·³éå¤ªé•·æˆ–åŒ…å«æ¨™é»çš„æ®µè½ï¼ˆå¯èƒ½æ˜¯æ­£æ–‡ï¼‰
        if len(para) > 30 or re.search(r'[.,;:]', para):
            continue

        normalized = para.lower()
        if normalized in start_keywords:
            return paragraphs[i + 1:], para  # âœ… å›å‚³æ®µè½å’Œé—œéµå­—æœ¬èº«

    return [], None


# ========== åµæ¸¬æ ¼å¼ ==========
def detect_reference_style(ref_text):
    # IEEE é€šå¸¸é–‹é ­æ˜¯ [1]ï¼Œæˆ–å«æœ‰è‹±æ–‡å¼•è™Ÿ "æ¨™é¡Œ"
    if re.match(r'^\[\d+\]', ref_text) or '"' in ref_text:
        return "IEEE"
    # APA å¸¸è¦‹çµæ§‹ï¼šä½œè€…ï¼ˆè¥¿å…ƒå¹´ï¼‰ã€‚æ¨™é¡Œã€‚
    if re.search(r'\((\d{4}[a-c]?|n\.d\.)\)\.', ref_text, re.IGNORECASE):
        return "APA"
    return "Unknown"

# ========== æ®µè½åˆä½µå™¨ï¼ˆPDF å°ˆç”¨ï¼Œæ ¹æ“šåƒè€ƒæ–‡ç»é–‹é ­åˆ‡åˆ†ï¼‰ ==========

def is_reference_head(para):
    """
    åˆ¤æ–·æ®µè½æ˜¯å¦ç‚ºåƒè€ƒæ–‡ç»é–‹é ­ï¼ˆAPA æˆ– IEEEï¼‰
    APA æ¢ä»¶ï¼šæ®µè½ä¸­å‡ºç¾ (XXXX). ä¸”å¾Œé¢ç‚º . ç©ºç™½
    IEEE æ¢ä»¶ï¼šé–‹é ­ç‚º [æ•¸å­—]
    """

    # APAï¼šå…è¨±ä»»ä½• 4 ä½æ•¸å­—æˆ– n.d.ï¼Œä½†å¾Œé¢å¿…é ˆæ˜¯ . ç©ºç™½ï¼ˆç¬¦åˆ APA æ ¼å¼ï¼‰
    if re.search(r"\((\d{4}[a-c]?|n\.d\.)\)\.\s", para, re.IGNORECASE):
        return True

    # IEEEï¼šé–‹é ­ç‚º [æ•¸å­—]
    if re.match(r"^\[\d+\]", para):
        return True

    return False

def merge_references_by_heads(paragraphs):
    merged = []

    for para in paragraphs:
        # è‹¥åŒä¸€æ®µä¸­æœ‰å¤šå€‹ APA å¹´ä»½å‡ºç¾ï¼Œå…ˆå˜—è©¦åˆ†æ®µ
        if len(re.findall(r'\(\d{4}[a-c]?\)', para)) >= 2:
            sub_refs = split_multiple_apa_in_paragraph(para)
            
            # âœ… æ—¢ç„¶æ˜¯æˆ‘å€‘åˆ‡å‡ºä¾†çš„ï¼Œå°±å…¨éƒ¨è¦–ç‚ºç¨ç«‹æ–‡ç»
            merged.extend([s.strip() for s in sub_refs if s.strip()])

        else:
            # åªæœ‰ä¸€ç¯‡æ™‚ï¼Œå†æ­£å¸¸ä¾æ“šé–‹é ­é€²è¡Œåˆä½µåˆ¤æ–·
            if is_reference_head(para):
                merged.append(para.strip())
            else:
                if merged:
                    merged[-1] += " " + para.strip()
                else:
                    merged.append(para.strip())

    return merged


#åˆä½µéŒ¯èª¤çš„æª¢æŸ¥ å¯èƒ½æœƒéœ€äºŒæ¬¡åˆ†å‰²
def split_multiple_apa_in_paragraph(paragraph):
    """
    æ”¹è‰¯ç‰ˆï¼šå¾å‡ºç¾ç¬¬ 2 ç­† (å¹´ä»½) èµ·ï¼Œå¾€å‰å°‹æ‰¾ `X. (å¹´ä»½)` çš„é–‹é ­ä½œç‚ºåˆ‡åˆ†é»ã€‚
    å…·é«”åšæ³•ï¼šæœå°‹ `. (199X)` å‰ä¸€å€‹å­—å…ƒï¼Œä½œç‚ºåˆ‡é»ï¼Œç¢ºä¿æ–°æ®µè½å¾ä½œè€…ç¸®å¯«é–‹å§‹ã€‚
    """
    matches = list(re.finditer(r'\((\d{4}[a-z]?|n\.d\.)\)\.', paragraph, re.IGNORECASE))
    if len(matches) < 2:
        return [paragraph]

    split_indices = []

    for i in range(1, len(matches)):
        year_pos = matches[i].start()
        # å›æº¯è‡³ ". " å†å¾€å‰ 1 å€‹å­—å…ƒ
        lookback_window = paragraph[max(0, year_pos - 10):year_pos]
        dot_space_match = re.search(r'([A-Z]\.)\s$', lookback_window)
        if dot_space_match:
            cut_offset = year_pos - (len(lookback_window) - dot_space_match.start(1))
            split_indices.append(cut_offset)
        else:
            # fallbackï¼šå¦‚æ‰¾ä¸åˆ°ï¼Œä»ä»¥å¹´ä»½èµ·é»ç‚ºåˆ‡é»
            split_indices.append(year_pos)

    # å¯¦éš›åˆ†æ®µ
    segments = []
    start = 0
    for idx in split_indices:
        segments.append(paragraph[start:idx].strip())
        start = idx
    segments.append(paragraph[start:].strip())

    return [s for s in segments if s]



# ========== æ“·å–æ¨™é¡Œ ==========
def extract_title(ref_text, style):
    if style == "APA":
        match = re.search(r'\((\d{4}[a-c]?|n\.d\.)\)\.\s(.+?)(?:\.\s|$)', ref_text, re.IGNORECASE)
        if match:
            return match.group(2).strip()
    elif style == "IEEE":
        matches = re.findall(r'"([^"]+)"', ref_text)
        if matches:
            return max(matches, key=len).strip().rstrip(",.")
        else:
            fallback = re.search(r'(?<!et al)([A-Z][^,.]+[a-zA-Z])[,\.]', ref_text)
            if fallback:
                return fallback.group(1).strip(" ,.")
    return None

# ========== Streamlit UI ==========
st.set_page_config(page_title="Reference Checker", layout="centered")
if "start_query" not in st.session_state:
    st.session_state.start_query = False
if "query_results" not in st.session_state:
    st.session_state.query_results = None
st.title("ğŸ“š Reference Checker")

st.markdown("""
<div style="background-color: #fff9db; padding: 15px; border-left: 6px solid #f1c40f; border-radius: 6px;">
    <span style="font-size: 16px; font-weight: bold;">æ³¨æ„äº‹é …</span><br>
    <span style="font-size: 15px; color: #444;">
    ç‚ºç¯€çœæ ¸å°æ™‚é–“ï¼Œæœ¬ç³»çµ±åªæŸ¥å°æœ‰ DOI ç¢¼çš„æœŸåˆŠè«–æ–‡ã€‚ä¸¦æœªæª¢æŸ¥æœŸåˆŠåç¨±ã€ä½œè€…ã€å·æœŸã€é ç¢¼ï¼Œåƒ…é‡å°ç¯‡åé€²è¡Œæ ¸å°ã€‚æœ¬ç³»çµ±åƒ…æä¾›åˆæ­¥ç¯©é¸åƒè€ƒï¼Œæ¯”å°å¾Œæ‡‰é€²è¡Œäººå·¥æ ¸å°ï¼Œä¸å¾—ç›´æ¥ä»¥æœ¬ç³»çµ±æ ¸å°çµæœä½œç‚ºå­¸è¡“å€«ç†åˆ¤æ–·çš„ä¾æ“šã€‚
    </span>
</div>
""", unsafe_allow_html=True)
st.markdown(" ")

uploaded_files = st.file_uploader("è«‹ä¸Šå‚³æœ€å¤š 10 å€‹ Word æˆ– PDF æª”æ¡ˆ", type=["docx", "pdf"], accept_multiple_files=True)
# æ””æˆªè¶…é 10 æª”æ¡ˆçš„æƒ…æ³
if uploaded_files and len(uploaded_files) > 10:
    st.error("âŒ ä¸Šå‚³æª”æ¡ˆè¶…é 10 å€‹ï¼Œè«‹åˆªé™¤éƒ¨åˆ†æª”æ¡ˆå¾Œå†è©¦ä¸€æ¬¡ã€‚")
    st.stop()

start_button = st.button("ğŸš€ é–‹å§‹æŸ¥è©¢")

if uploaded_files and start_button:
    st.subheader("ğŸ“Š æ­£åœ¨æŸ¥è©¢ä¸­ï¼Œè«‹ç¨å€™...")

    all_results = []

    for uploaded_file in uploaded_files:
        file_ext = uploaded_file.name.split(".")[-1].lower()
        st.markdown(f"ğŸ“„ è™•ç†æª”æ¡ˆï¼š {uploaded_file.name}")

        # é¡¯ç¤ºç¨ç«‹é€²åº¦æ¢ï¼ˆè¦å¯«åœ¨æª”æ¡ˆ for è¿´åœˆå…§ï¼‰
        file_progress = st.progress(0.0)

        # æª”æ¡ˆè§£æ
        if file_ext == "docx":
            paragraphs = extract_paragraphs_from_docx(uploaded_file)
            skip_section_detection = False
        elif file_ext == "pdf":
            paragraphs = extract_paragraphs_from_pdf(uploaded_file)
            skip_section_detection = False
            
        else:
            st.warning(f"âš ï¸ æª”æ¡ˆ {uploaded_file.name} æ ¼å¼ä¸æ”¯æ´ï¼Œå°‡ç•¥éã€‚")
            continue


        # åµæ¸¬åƒè€ƒæ–‡ç»æ®µè½
        matched_section = []
        if not skip_section_detection:
            matched_section, matched_keyword = extract_reference_section_from_bottom(paragraphs)
            # fallbackï¼šå¦‚æœæ²’æ‰¾åˆ°ä»»ä½•ç¬¦åˆçš„é—œéµå­—æ®µè½ï¼Œå°±ç›´æ¥ç”¨æ•´ä»½è™•ç†
            if not matched_section:
                st.warning(f"âš ï¸ æª”æ¡ˆ {uploaded_file.name} æœªåµæ¸¬åˆ°åƒè€ƒæ–‡ç»æ¨™é¡Œï¼Œå°‡å˜—è©¦ä»¥å…¨æ–‡è™•ç†ã€‚")
                matched_section = paragraphs
        else:
            matched_section = paragraphs

        with st.expander("æ“·å–åˆ°çš„åƒè€ƒæ–‡ç»æ®µè½ï¼ˆä¾›äººå·¥æª¢æŸ¥ï¼‰"):
            if matched_keyword:
                st.markdown(f"ğŸ” åµæ¸¬åˆ°åƒè€ƒæ–‡ç»èµ·é»é—œéµå­—ç‚ºï¼š**{matched_keyword}**")
            else:
                st.markdown("ğŸ” æœªåµæ¸¬åˆ°ç‰¹å®šé—œéµå­—ï¼Œæ”¹ä»¥æ•´ä»½æ–‡ä»¶è™•ç†ã€‚")

            for i, para in enumerate(matched_section, 1):
                st.markdown(f"**{i}.** {para}")

        
        # åˆä½µ PDF åˆ†æ®µåƒè€ƒæ–‡ç»ï¼ˆä½¿ç”¨çµ±ä¸€çš„ã€Œé–‹é ­åˆä½µæ³•ã€ï¼‰
        if file_ext == "pdf":
            merged_references = merge_references_by_heads(matched_section)
        else:
            merged_references = matched_section




        # æ”¹ç‚ºä½¿ç”¨ merged_references è™•ç†æ¯ç­†æ–‡ç»
        title_pairs = []
        with st.expander("é€ç­†åƒè€ƒæ–‡ç»è§£æçµæœï¼ˆåˆä½µå¾Œæ®µè½ + æ¨™é¡Œ + DOI + æ ¼å¼ï¼‰"):
            ref_index = 1
            for para in merged_references:
                # è‹¥åŒæ®µåŒ…å«å¤šå€‹ APA å¹´ä»½ï¼Œå…ˆå¼·åˆ¶åˆ†æ®µè™•ç†
                year_matches = list(re.finditer(r'\(\d{4}[a-c]?\)', para))
                if len(year_matches) >= 2:
                    sub_refs = split_multiple_apa_in_paragraph(para)
                    st.markdown(f"ğŸ” å¼·åˆ¶åˆ‡åˆ†æ®µè½ï¼ˆåŸå§‹æ®µè½å« {len(year_matches)} å¹´ä»½ï¼‰ï¼š")
                    for i, sub_ref in enumerate(sub_refs, 1):
                        style = detect_reference_style(sub_ref)
                        title = extract_title(sub_ref, style)
                        doi = extract_doi(sub_ref)

                        highlights = sub_ref
                        for match in reversed(list(re.finditer(r'\(\d{4}[a-c]?\)', sub_ref))):
                            start, end = match.span()
                            highlights = highlights[:start] + "**" + highlights[start:end] + "**" + highlights[end:]


                        st.markdown(f"**{ref_index}.**")
                        st.write(highlights)
                        st.markdown(f"""
                        â€¢ ğŸ“° **æ“·å–æ¨™é¡Œ**ï¼š{title if title else "âŒ ç„¡æ³•æ“·å–"}  
                        â€¢ ğŸ” **æ“·å– DOI**ï¼š{doi if doi else "âŒ ç„¡ DOI"}  
                        â€¢ ğŸ·ï¸ **åµæ¸¬é¢¨æ ¼**ï¼š`{style}`  
                        â€¢ ğŸ“… **å¹´ä»½å‡ºç¾æ¬¡æ•¸**ï¼š{len(re.findall(r'\(\d{4}[a-c]?\)', sub_ref))}  
                        """)
                        if title:
                            title_pairs.append((sub_ref, title))
                        ref_index += 1
                else:
                    ref = para
                    style = detect_reference_style(ref)
                    title = extract_title(ref, style)
                    doi = extract_doi(ref)

                    highlights = ref
                    for match in reversed(list(re.finditer(r'\(\d{4}[a-c]?\)', ref))):
                        start, end = match.span()
                        highlights = highlights[:start] + "**" + highlights[start:end] + "**" + highlights[end:]

                    st.markdown(f"**{ref_index}.**")
                    st.write(highlights)
                    st.markdown(f"""
                    â€¢ ğŸ“° **æ“·å–æ¨™é¡Œ**ï¼š{title if title else "âŒ ç„¡æ³•æ“·å–"}  
                    â€¢ ğŸ” **æ“·å– DOI**ï¼š{doi if doi else "âŒ ç„¡ DOI"}  
                    â€¢ ğŸ·ï¸ **åµæ¸¬é¢¨æ ¼**ï¼š`{style}`  
                    â€¢ ğŸ“… **å¹´ä»½å‡ºç¾æ¬¡æ•¸**ï¼š{len(re.findall(r'\((\d{4}[a-c]?|n\.d\.)\)', ref, re.IGNORECASE))}  
                    """)
                    if title:
                        title_pairs.append((ref, title))
                    ref_index += 1



        # æŸ¥è©¢è™•ç†
        crossref_doi_hits = {}
        scopus_hits = {}
        scholar_hits = {}
        scholar_similar = {}
        not_found = []

        for i, (ref, title) in enumerate(title_pairs, 1):
            doi = extract_doi(ref)
            if doi:
                title_from_doi, url = search_crossref_by_doi(doi)
                if title_from_doi:
                    crossref_doi_hits[ref] = url
                    file_progress.progress(i / len(title_pairs))
                    continue

            url = search_scopus_by_title(title)
            if url:
                scopus_hits[ref] = url
            else:
                gs_url, gs_type = search_scholar_by_title(title, SERPAPI_KEY)
                if gs_type == "match":
                    scholar_hits[ref] = gs_url
                elif gs_type == "similar":
                    scholar_similar[ref] = gs_url
                else:
                    not_found.append(ref)

            file_progress.progress(i / len(title_pairs))

        file_results = {
            "filename": uploaded_file.name,
            "title_pairs": title_pairs,
            "crossref_doi_hits": crossref_doi_hits,
            "scopus_hits": scopus_hits,
            "scholar_hits": scholar_hits,
            "scholar_similar": scholar_similar,
            "not_found": not_found,
            "report_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        all_results.append(file_results)

    st.session_state.query_results = all_results

# å¦‚æœ SerpAPI ç”¨é‡å·²è¶…éï¼Œé¡¯ç¤ºä¸€æ¬¡æ€§æç¤º
if st.session_state.get("serpapi_exceeded"):
    st.warning("âš ï¸ SerpAPI æŸ¥è©¢é¡åº¦å·²ç”¨å®Œï¼Œå› æ­¤éƒ¨åˆ†çµæœå¯èƒ½ç„¡æ³•å¾ Google Scholar æŸ¥å¾—ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–ç¢ºèª API ä½¿ç”¨ç‹€æ³ã€‚")
elif st.session_state.get("serpapi_error"):
    st.warning(f"âš ï¸ Google Scholar æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{st.session_state['serpapi_error']}")




# ========== ä¸Šå‚³ä¸¦è™•ç† ==========


if st.session_state.query_results:
        st.markdown("---")
        st.subheader("ğŸ“Š æŸ¥è©¢çµæœåˆ†é¡")
        for result in st.session_state.query_results:
            not_found = result["not_found"]
            title_pairs = result["title_pairs"]
            crossref_doi_hits = result["crossref_doi_hits"]
            scholar_similar = result["scholar_similar"]
            uploaded_filename = result["filename"]
            report_time = result["report_time"]
            scopus_hits = result["scopus_hits"]
            scholar_hits = result["scholar_hits"]

            st.markdown(f"ğŸ“„ æª”æ¡ˆåç¨±ï¼š {uploaded_filename}")
            matched_count = len(crossref_doi_hits) + len(scopus_hits) + len(scholar_hits)
            hit_tab, similar_tab, miss_tab = st.tabs([
                f"ğŸŸ¢ å‘½ä¸­çµæœï¼ˆ{matched_count}ï¼‰",
                f"ğŸŸ¡ Google Scholar é¡ä¼¼æ¨™é¡Œï¼ˆ{len(scholar_similar)}ï¼‰",
                f"ğŸ”´ å‡æŸ¥ç„¡çµæœï¼ˆ{len(not_found)}ï¼‰"
            ])

            with hit_tab:
                if crossref_doi_hits:
                    with st.expander(f"\U0001F7E2 Crossref DOI å‘½ä¸­ï¼ˆ{len(crossref_doi_hits)}ï¼‰"):
                        for i, (title, url) in enumerate(crossref_doi_hits.items(), 1):
                            st.markdown(f"{i}. {title}  \nğŸ”— [DOI é€£çµ]({url})", unsafe_allow_html=True)

                if scopus_hits:
                    with st.expander(f"\U0001F7E2 Scopus æ¨™é¡Œå‘½ä¸­ï¼ˆ{len(scopus_hits)}ï¼‰"):
                        for i, (title, url) in enumerate(scopus_hits.items(), 1):
                            st.markdown(f"{i}. {title}  \nğŸ”— [Scopus é€£çµ]({url})", unsafe_allow_html=True)

                if scholar_hits:
                    with st.expander(f"\U0001F7E2 Google Scholar æ¨™é¡Œå‘½ä¸­ï¼ˆ{len(scholar_hits)}ï¼‰"):
                        for i, (title, url) in enumerate(scholar_hits.items(), 1):
                            st.markdown(f"{i}. {title}  \nğŸ”— [Scholar é€£çµ]({url})", unsafe_allow_html=True)

                if not (crossref_doi_hits or scopus_hits or scholar_hits):
                    st.info("æ²’æœ‰å‘½ä¸­ä»»ä½•åƒè€ƒæ–‡ç»ã€‚")

            with similar_tab:
                if scholar_similar:
                    for i, (title, url) in enumerate(scholar_similar.items(), 1):
                        with st.expander(f"{i}. {title}"):
                            st.markdown(f"ğŸ”— [Google Scholar çµæœé€£çµ]({url})", unsafe_allow_html=True)
                            st.warning("âš ï¸ æ­¤ç‚ºç›¸ä¼¼æ¨™é¡Œï¼Œè«‹äººå·¥ç¢ºèªæ˜¯å¦ç‚ºæ­£ç¢ºæ–‡ç»ã€‚")
                else:
                    st.info("ç„¡æ¨™é¡Œç›¸ä¼¼ä½†ä¸ä¸€è‡´çš„çµæœã€‚")

            with miss_tab:
                if not_found:
                    for i, title in enumerate(not_found, 1):
                        scholar_url = f"https://scholar.google.com/scholar?q={urllib.parse.quote(title)}"
                        st.markdown(f"{i}. {title}  \nğŸ”— [Google Scholar æœå°‹]({scholar_url})", unsafe_allow_html=True)
                    st.markdown("ğŸ‘‰ è«‹è€ƒæ…®æ‰‹å‹•æœå°‹ Google Scholarã€‚")
                else:
                    st.success("æ‰€æœ‰æ¨™é¡Œçš†æˆåŠŸæŸ¥è©¢ï¼")

        # ä¸‹è¼‰çµæœ
        st.markdown("---")

        export_data = []
        for result in st.session_state.query_results:
            filename = result["filename"]
            for ref, title in result["title_pairs"]:
                if ref in result["crossref_doi_hits"]:
                    export_data.append([filename, ref, "Crossref æœ‰ DOI è³‡è¨Š", result["crossref_doi_hits"][ref]])
                elif ref in result["scopus_hits"]:
                    export_data.append([filename, ref, "æ¨™é¡Œå‘½ä¸­ï¼ˆScopusï¼‰", result["scopus_hits"][ref]])
                elif ref in result["scholar_hits"]:
                    export_data.append([filename, ref, "æ¨™é¡Œå‘½ä¸­ï¼ˆGoogle Scholarï¼‰", result["scholar_hits"][ref]])
                elif ref in result["scholar_similar"]:
                    export_data.append([filename, ref, "Google Scholar é¡ä¼¼æ¨™é¡Œ", result["scholar_similar"][ref]])
                elif ref in result["not_found"]:
                    scholar_url = f"https://scholar.google.com/scholar?q={urllib.parse.quote(ref)}"
                    export_data.append([filename, ref, "æŸ¥ç„¡çµæœ", scholar_url])
        total_refs = sum(len(r["title_pairs"]) for r in st.session_state.query_results)
        matched_exact = sum(len(r["crossref_doi_hits"]) + len(r["scopus_hits"]) + len(r["scholar_hits"]) for r in st.session_state.query_results)
        matched_similar = sum(len(r["scholar_similar"]) for r in st.session_state.query_results)
        unmatched = sum(len(r["not_found"]) for r in st.session_state.query_results)

        header = StringIO()
        header.write(f"å ±å‘Šç”¢å‡ºæ™‚é–“ï¼š{report_time}\n\n")
        header.write("èªªæ˜ï¼š\n")
        header.write("ç‚ºç¯€çœæ ¸å°æ™‚é–“ï¼Œæœ¬ç³»çµ±åªæŸ¥å°æœ‰DOIç¢¼çš„æœŸåˆŠè«–æ–‡ã€‚ä¸”ä¸¦æœªæª¢æŸ¥æœŸåˆŠåç¨±ã€ä½œè€…ã€å·æœŸã€é ç¢¼ã€‚åªé‡å°ç¯‡åé€²è¡Œæ ¸å°ã€‚\n")
        header.write("æœ¬ç³»çµ±åªæ˜¯ç‚ºäº†æä¾›åˆæ­¥ç¯©é¸ï¼Œæ¯”å°å¾Œæ‡‰æ¥è‘—é€²è¡Œäººå·¥æ ¸å°ï¼Œä»»ä½•äººéƒ½ä¸æ‡‰è©²ä»¥æœ¬ç³»çµ±æ ¸å°çµæœä½œç‚ºä»»ä½•å­¸è¡“å€«ç†åˆ¤æ–·ä¹‹åŸºç¤ã€‚\n\n")

        csv_buffer = StringIO()
        csv_buffer.write(header.getvalue())
        if not export_data:
            st.warning("âš ï¸ æ²’æœ‰å¯åŒ¯å‡ºçš„æŸ¥æ ¸çµæœã€‚")
        else:
            df_export = pd.DataFrame(export_data, columns=["æª”æ¡ˆåç¨±", "åŸå§‹åƒè€ƒæ–‡ç»", "æŸ¥æ ¸çµæœ", "é€£çµ"])
            df_export.to_csv(csv_buffer, index=False)

        # çµ±è¨ˆæ‰€æœ‰æª”æ¡ˆçš„ç¸½æ•¸
        total_files = len(st.session_state.query_results)
        total_refs = sum(len(r["title_pairs"]) for r in st.session_state.query_results)
        matched_crossref = sum(len(r["crossref_doi_hits"]) for r in st.session_state.query_results)
        matched_scopus = sum(len(r["scopus_hits"]) for r in st.session_state.query_results)
        matched_scholar = sum(len(r["scholar_hits"]) for r in st.session_state.query_results)
        matched_similar = sum(len(r["scholar_similar"]) for r in st.session_state.query_results)
        matched_notfound = sum(len(r["not_found"]) for r in st.session_state.query_results)


        st.markdown(f"""
        ğŸ“Œ æŸ¥æ ¸çµæœèªªæ˜ï¼šæœ¬æ¬¡å…±è™•ç† **{total_files} ç¯‡è«–æ–‡**ï¼Œç¸½å…±æ“·å– **{total_refs} ç¯‡åƒè€ƒæ–‡ç»**ï¼Œå…¶ä¸­ï¼š

        - {matched_crossref} ç¯‡ç‚ºã€ŒCrossref æœ‰ DOI è³‡è¨Šã€
        - {matched_scopus} ç¯‡ç‚ºã€Œæ¨™é¡Œå‘½ä¸­ï¼ˆScopusï¼‰ã€
        - {matched_scholar} ç¯‡ç‚ºã€Œæ¨™é¡Œå‘½ä¸­ï¼ˆGoogle Scholarï¼‰ã€
        - {matched_similar} ç¯‡ç‚ºã€ŒGoogle Scholar é¡ä¼¼æ¨™é¡Œã€
        - {matched_notfound} ç¯‡ç‚ºã€ŒæŸ¥ç„¡çµæœã€
        """)
        st.markdown("---")
        
        st.subheader("ğŸ“¥ ä¸‹è¼‰æŸ¥è©¢çµæœ")

        st.download_button(
            label="ğŸ“¤ ä¸‹è¼‰çµæœ CSV æª”",
            data=csv_buffer.getvalue().encode('utf-8-sig'),
            file_name="reference_results.csv",
            mime="text/csv"
        )
        st.write("ğŸ” è‹¥è¦é‡æ–°ä¸Šå‚³æª”æ¡ˆï¼Œè«‹æŒ‰ä¸‹éµç›¤ä¸Šçš„ F5 æˆ–é»æ“Šç€è¦½å™¨é‡æ–°æ•´ç†æŒ‰éˆ•")    